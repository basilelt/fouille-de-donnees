# Notes on Data Mining - Neural Networks

## Main Takeaway

This document provides an overview of Neural Networks and Deep Learning, covering their history, the fundamental concept of the perceptron and its learning algorithm, and the evolution of deep neural network architectures. The core idea of adjusting weights to correct errors in perceptrons forms the foundation for modern, multi-layer neural networks, which are crucial for solving complex, non-linear problems across various applications.

## 1. Neural Networks

**Source:** Germain Forestier, PhD, Université de Haute-Alsace (https://germain-forestier.info)

### History

- 1960s: Rosenblatt programmed a perceptron (binary classifier).
- 1969: Minsky and Papert highlighted the problem of nonlinear decision boundaries.
- 1970s: AI Winter.
- 1974: Werbos proposed the backpropagation algorithm in his thesis.
- 1986: Rumelhart, Hinton, and Williams rediscovered the backpropagation algorithm.
- 1990s: Convolutional neural networks by Yann LeCun.

**Source:** https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html

### Perceptron

#### Key Concepts

- Takes n input values (x1, ..., xn), with x0 = 1 (bias).
- Computes an output o.
- Parameters to learn are weights (w0, ..., wn).
- Output o is computed as Σ wi xi.
- This sum is passed through an activation function.
- Activation function: Heaviside function:

  ```
  f(x) = 1 if x > 0
         0 otherwise
  ```

- Perceptrons model linear decision boundaries but cannot solve non-linear problems.

#### Structure Diagram

```
x0 ---- w0 ----
x1 ---- w1 ----
x2 ---- w2 ----
...             
xn ---- wn ---- > Σ > Activation function > Output (o)
```

- Entry weights are learned parameters.

### Perceptron Learning Algorithm

#### "Classic" Algorithm

1. Initialize perceptron weights to arbitrary values.
2. Each time a new example is presented, adjust weights based on correct/incorrect classification.
3. Algorithm stops when all examples are presented without any weight adjustments.

#### Error Correction Algorithm

- **Input:** Set of examples (x1, ..., xn) and expected outputs (c).
- Random initialization of weights wi (for i from 0 to n).
- Repeat:
  - Take an example.
  - Compute perceptron output o for the example.
  - Weight update:
    - For i from 0 to n:
      - wi = wi + (c - o)xi
    - Endfor
  - Endrepeat
- **Output:** Perceptron defined by (w0, w1, ..., wn).
- **Risk:** Oscillation; requires a defined number of epochs.

#### Perceptron Learning Example: Boolean OR

- Initialization: w0 = 0, w1 = 1, w2 = -1.
- Examples Order: (1,0,0,0), (1,0,1,1), (1,1,0,1), (1,1,1,1) (where x0, x1, x2, c).
- The document provides a detailed step-by-step unrolling of the learning process, showing weight adjustments over 10 iterations. The weights adjust based on the error (c - o) until the perceptron converges to correctly classify all OR inputs.

## 2. Deep Learning

### Definition

Deep Learning ≡ Deep Neural Networks ≡ Multi-layer Networks.

### Success

Lies in training multi-layer neural networks to learn nonlinear decisions.

"Deep Learning: a fancy word for deep neural networks."

### Key Deep Neural Network Architectures

#### LeNet-5 (1998): For MNIST-like tasks (document recognition).

**Features:**

- Designed for grayscale images.
- Convolutions followed by pooling, then a final MLP.
- Uses linear functions: sigmoid/tanh.
- Activation functions applied after pooling.
- Same filters applied to each channel.
- Contains 60,000 learnable parameters.

#### AlexNet (2012): For ImageNet.

**Features:**

- Designed for RGB images.
- Similar to LeNet-5 but significantly larger.
- Contains 60 million parameters.
- Uses the ReLU (Rectified Linear Unit) activation function.
- Had the most significant impact on deep learning.

#### Inception (2014): For ImageNet.

**Features:**

- Implements 1x1 convolution to reduce computational cost.
- Uses multiple filter sizes in parallel.
- Eliminates the need to manually choose these hyperparameters.

#### VGG-16 (2015): For ImageNet.

**Features:**

- Same filter size (3x3) for all layers.
- Same max-pooling size (2x2) for all layers.
- Contains 138 million learnable parameters.
- No need to manually choose filters or pooling sizes.

## 3. Conclusion

### Key Points

- The concept of training perceptrons (adjusting weights to correct errors) is the fundamental principle of modern neural networks.
- Deep learning extends this by using multi-layer networks to solve more complex problems.
- The same training approach applies across various applications (e.g., image processing, speech recognition, language models like GPT).