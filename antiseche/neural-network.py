## Neural Networks

- The concept of training perceptrons—adjusting weights to correct errors—remains the foundation of modern neural networks.
- Deep learning extends this principle by using multi-layer networks to solve more complex problems.
- The same training approach is used across various applications, including image processing, speech recognition, and language models like GPT.


Who developed the perceptron in the 1960s?
Rosenblatt


What is the main limitation of perceptrons?
Cannot solve non-linear problems


Who proposed the backpropagation algorithm in 1974?
Werbos


What activation function was initially used in perceptrons?
Heaviside


What is a key feature of LeNet-5?
Designed for grayscale images


What distinguishes AlexNet from LeNet-5?
Handles RGB images


Which model introduced the concept of parallel filter sizes?
Inception


How many learnable parameters does VGG-16 have?
138 million


What principle is foundational to neural network training?
Error correction


What technique does Inception use to reduce computational cost?
1x1 convolutions


What activation function is commonly used in AlexNet?
ReLU


What was a major achievement of AlexNet in 2012?
Popularized deep learning


What is the primary advantage of deep neural networks?
Non-linear decision boundaries


Who highlighted the problem of non-linear decision boundaries?
Minsky and Papert


What is the output of a perceptron passed through?
Activation function