# Decision Trees: Concepts, Construction, and Overfitting

Decision Trees are intuitive, flowchart-like models used for classification and regression tasks, easily visualized and interpreted. They are built by recursively splitting data based on attributes to reduce uncertainty, but can be prone to overfitting.

## 1. Introduction and Motivation

- **Definition**: A flowchart-like tree structure where:
  - Each internal node denotes a test on an attribute.
  - Each branch represents an outcome of the test.
  - Each leaf node holds a class label or continuous value (decision).
- **Purpose**: Used for both classification and regression.
- **Structure**:
  - Built from data.
  - Nodes: attributes.
  - Branches: values.
  - Leaves: decisions (classes).
- **Classification Process**: A new instance is tested by its path from the root to a leaf node.

## 2. ID3 (Iterative Dichotomiser 3)

- **Developer**: Ross Quinlan in 1986.
- **Purpose**: Create a decision tree to classify instances.
- **Approach**:
  - Top-down, greedy search through possible branches (no backtracking).
  - Selects the attribute that is most informative (highest information gain) as the decision node, proceeding recursively.
- **Attribute Selection**: Based on entropy and information gain.
- **Limitations**:
  - Can overfit the data.
  - Biased towards attributes with many outcomes.
  - Does not handle numeric attributes or missing values directly.

## 3. Criteria for Splitting

### Entropy Heuristic

- **Measures**: The amount of information or uncertainty in a set of examples (E).
- **Formula**:
  $$
  H(E) = -\sum (p_i \cdot \log_2(p_i))
  $$
  Where \( p_i \) is the proportion of examples of class \( i \) in set \( E \).
- **Interpretation**:
  - Maximal Uncertainty: If classes are equally distributed (e.g., 4 yes, 4 no out of 8), \( H(E) = 1 \).
  - Zero Uncertainty: If all examples belong to one class (e.g., 8 yes out of 8), \( H(E) = 0 \).
  - Higher entropy implies greater uncertainty; lower entropy implies less uncertainty.

### Entropy Gain (Information Gain)

- **Measures**: Reduction in entropy (uncertainty) caused by partitioning a set of examples according to an attribute.
- **Formula**:
  $$
  G(a, E) = H(E) - \sum \left( \frac{|E_{a,v}|}{|E|} \cdot H(E_{a,v}) \right)
  $$
  Where:
  - \( E \) is the set of examples.
  - \( a \) is the attribute.
  - \( V(a) \) is the set of values for attribute \( a \).
  - \( E_{a,v} \) is the subset of \( E \) where attribute \( a \) has value \( v \).
- **Objective**: The ID3 algorithm selects the attribute with the highest information gain to split the node.
- **Example Calculations**:
  - Initial population: 9 Yes, 5 No.
  - \( H(E) \approx 0.94 \)
  - **Attribute Sky**:
    - Sunny (2 Yes, 3 No): \( H(\text{sunny}) \approx 0.97 \)
    - Overcast (4 Yes, 0 No): \( H(\text{overcast}) = 0 \)
    - Rain (3 Yes, 2 No): \( H(\text{rain}) \approx 0.97 \)
    - \( G(\text{sky}, E) \approx 0.94 - \left( \frac{5}{14} \cdot 0.97 + \frac{4}{14} \cdot 0 + \frac{5}{14} \cdot 0.97 \right) \approx 0.246 \)
  - **Attribute Humidity**:
    - High (3 Yes, 4 No): \( H(\text{high}) \approx 0.98 \)
    - Normal (6 Yes, 1 No): \( H(\text{normal}) \approx 0.59 \)
    - \( G(\text{humidity}, E) \approx 0.94 - \left( \frac{7}{14} \cdot 0.98 + \frac{7}{14} \cdot 0.59 \right) \approx 0.151 \)
  - **Attribute Wind**:
    - Strong (3 Yes, 3 No): \( H(\text{strong}) = 1 \)
    - Weak (6 Yes, 2 No): \( H(\text{weak}) \approx 0.81 \)
    - \( G(\text{wind}, E) \approx 0.94 - \left( \frac{6}{14} \cdot 1 + \frac{8}{14} \cdot 0.81 \right) \approx 0.048 \)
  - **Conclusion**: Sky has the highest gain (0.246), so it would be chosen as the root node.

## 4. Handling Numerical Data

- **Challenge**: Cannot calculate gain for each continuous value directly.
- **Solution**: Discretize the variable (e.g., < 70 = Cold, [70-75] = Mild, > 75 = Hot).
- **Thresholds**: Can be found before or during tree construction (e.g., C4.5 algorithm).

## 5. Overfitting in Decision Trees

- **Definition**: Occurs when a model learns the training data too well, including noise and outliers, leading to poor generalization on new data.
- **Causes**:
  - Depth: Trees allowed to grow too deep create overly specific rules.
  - Complexity: Small splits deep in the tree capture noise.
  - Insufficient Data: Limited data leads to patterns that don't generalize.
  - No Pruning: Lack of pruning mechanisms.

### Pruning Strategies to Avoid Overfitting

- **Pruning**: Reducing tree size by converting branch nodes into leaf nodes, improving generalization.
- **Main Strategies**:
  - **Reduced Error Pruning**: Remove a node if it doesn't decrease prediction accuracy on a validation set.
  - **Cost Complexity Pruning (CCP)**: Introduces a penalty for complexity (number of leaf nodes) to minimize misclassification rate + penalty.
  - **Minimum Description Length (MDL)**: Prune based on Occam's razor (simplest model that fits the data).
  - **Minimum Error Pruning**: Prune nodes whose removal would not significantly increase the error rate.

## 6. Random Forest

- **Concept**: Builds multiple decision trees using subsets of the data.
- **Mechanism**: Averages the predictions of individual trees for the final decision.
- **Benefits**: Helps to reduce overfitting and can assess attribute importance.

## 7. Advantages and Disadvantages of Decision Trees

### Advantages

- **Interpretable**: Easy to understand and visualize.
- **Minimal Data Prep**: No need for normalization or scaling.
- **Handle Multiple Types**: Can deal with numerical and categorical data.
- **Non-linear Relationships**: Naturally handles non-linearity.

### Disadvantages

- **Overfitting**: Can become too complex, capturing noise.
- **Instability**: Small data changes can lead to different tree structures.
- **Bias**: Biased towards features with more levels.
- **Locally Optimal**: Greedy algorithms might not find the globally optimal tree.