{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rncxK7CCFOsA"
      },
      "source": [
        "# Naive Bayes (Bayes)\n",
        "\n",
        "**author** : Germain Forestier (germain.forestier@uha.fr)\n",
        "\n",
        "Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. They are particularly known for their efficiency in text classification tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHRUW6mLI7hA"
      },
      "source": [
        "## **Exercice 1**: Naive Bayes using Scikit-learn\n",
        "\n",
        "This exercise will guide you through implementing a Naive Bayes classifier using Python and Scikit-learn. You'll learn to handle data loading, preprocessing, training a model, and evaluating its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-9tRNeeJQDB"
      },
      "source": [
        "### Part 1: Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wjefDYfJSyn"
      },
      "outputs": [],
      "source": [
        "# Import the necessary library\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: Load the dataset from the provided URL into a DataFrame named 'data'\n",
        "# Hint: Use pd.read_csv() with the URL https://germain-forestier.info/dataset/weather.csv\n",
        "\n",
        "# TODO: Display the first few rows of the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giw5Tj5SJm49"
      },
      "source": [
        "### Part 2: Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdTQVhgFJq_k"
      },
      "outputs": [],
      "source": [
        "# Import LabelEncoder from sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize a LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# TODO: Apply the encoder to each categorical column and replace the column with encoded data\n",
        "# Example: data['Outlook'] = encoder.fit_transform(data['Outlook'])\n",
        "\n",
        "# TODO: Display the transformed data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqxapQK1JzI7"
      },
      "source": [
        "### Part 3: Splitting the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF9hqBLEJ1iL"
      },
      "outputs": [],
      "source": [
        "# Import train_test_split from sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "X = data.drop('Play', axis=1)  # Features\n",
        "y = data['Play']               # Target\n",
        "\n",
        "# TODO: Split the data into training and testing sets with a test size of 20% and random_state=42\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VSzLKcGJ5q7"
      },
      "source": [
        "### Part 4: Training the Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiR4_b9lJ7-F"
      },
      "outputs": [],
      "source": [
        "# Import CategoricalNB from sklearn\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "\n",
        "# Initialize the Categorical Naive Bayes classifier\n",
        "model = CategoricalNB()\n",
        "\n",
        "# TODO: Train the model using the training data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r6LIXmqKGBi"
      },
      "source": [
        "###Â Part 5: Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJpGAh-5KISv"
      },
      "outputs": [],
      "source": [
        "# Import accuracy_score and confusion_matrix from sklearn\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# TODO: Make predictions on the test set and store in a variable 'predictions'\n",
        "\n",
        "# TODO: Calculate and print the accuracy of the model\n",
        "\n",
        "# TODO: Generate and display the confusion matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQVls1FEMfvq"
      },
      "source": [
        "## **Exercise** 2: Text Classification using Multinomial Naive Bayes\n",
        "\n",
        "This exercise introduces students to text classification using the Multinomial Naive Bayes classifier, a popular algorithm for text data that involves word counts as features. We will work with the 20 Newsgroups dataset, focusing on a subset of categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFIWj2NwMkWl"
      },
      "source": [
        "### Part 1: Data Overview\n",
        "\n",
        "Before diving into text classification, it's crucial to understand the dataset you'll be working with. The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups. In this part, you'll load the dataset and explore the different categories available, which will help you become familiar with the type of text data you'll classify.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ussEEGqdMpaX"
      },
      "outputs": [],
      "source": [
        "# Import necessary library to fetch data\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# TODO: Load data using fetch_20newsgroups and display the target names (categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX5I1R2yMzBv"
      },
      "source": [
        "### Part 2: Selecting Specific Categories\n",
        "\n",
        "Often, you may not need all the data available in a dataset for your specific task. For this exercise, we'll focus on a subset of categories to simplify the learning process and reduce computation time. You will learn how to fetch data for selected categories only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deGtH2E9M1fZ"
      },
      "outputs": [],
      "source": [
        "# Define the categories to focus on\n",
        "categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics']\n",
        "\n",
        "# TODO: Fetch the training and testing data for the selected categories\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnGnnBUJNRjP"
      },
      "source": [
        "### Part 3: Text Vectorization and Model Training\n",
        "\n",
        "Text data must be converted into a numerical format that machine learning models can understand. Using the TF-IDF vectorization, you will convert text documents into a matrix of TF-IDF features. Then, you will use these features to train a Multinomial Naive Bayes model, which is especially suited for text classification tasks where features represent word counts or frequencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA430iL6NVdP"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for text vectorization and Naive Bayes classifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# TODO: Create a pipeline that includes TfidfVectorizer and MultinomialNB\n",
        "\n",
        "# TODO: Train the model with the training data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYPiLzIcNoxj"
      },
      "source": [
        "### Part 4: Evaluating the Model\n",
        "\n",
        "After training a model, it's crucial to evaluate its performance to understand how well it performs on unseen data. In this part, you will use the test data to make predictions and then visualize the results using a confusion matrix. This will allow you to see not only the overall accuracy but also where the model makes mistakes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRVONEY-Np8k"
      },
      "outputs": [],
      "source": [
        "# Import necessary library for evaluation\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO: Predict labels for the test data\n",
        "\n",
        "# TODO: Generate and visualize the confusion matrix using seaborn's heatmap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpYWgtj4OKem"
      },
      "source": [
        "### Part 5: Using the Model for Prediction\n",
        "\n",
        "Now that the model is trained and evaluated, we can use it to predict the category of any new piece of text. This functionality is extremely useful in many applications, such as sorting emails, organizing documents, and moderating content. Below is a utility function that predicts the category of a given text string using our trained Multinomial Naive Bayes pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWE1XDKkOMMQ",
        "outputId": "51ef49f1-1b39-41c5-8824-cd18f8b49f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sci.space\n",
            "soc.religion.christian\n",
            "comp.graphics\n"
          ]
        }
      ],
      "source": [
        "# Utility function to predict the category of a text\n",
        "def predict_category(s, train=train, model=model):\n",
        "    pred = model.predict([s])\n",
        "    return train.target_names[pred[0]]\n",
        "\n",
        "# Test the function with some example strings\n",
        "print(predict_category('Meeting to discuss the launch of the new space satellite.'))\n",
        "print(predict_category('Join us this Sunday for the church anniversary service.'))\n",
        "print(predict_category('How to dertermine the screen resolution!'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWBnBA5AQBNC"
      },
      "source": [
        "## **Exercise** 3: Advanced Model Optimization and Comparison\n",
        "\n",
        "This exercise will go deeper into Bayes classification and will explore advanced model optimization and comparisons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZmW0GBlQGFP"
      },
      "source": [
        "### Part 1: Feature Engineering with N-Grams\n",
        "\n",
        "Introducing N-Grams into our feature set allows the model to consider the context provided by adjacent word sequences, potentially increasing its predictive accuracy. You will modify the TfidfVectorizer to include not only single words (uni-grams) but also sequences of two and three words (bi-grams and tri-grams).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P682a2s2QdGg"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for the classifier and vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# TODO: Create a TfidfVectorizer with ngram_range set to include uni-grams, bi-grams, and tri-grams\n",
        "# TODO: Integrate this vectorizer into a pipeline with the MultinomialNB classifier\n",
        "\n",
        "# Train the model with the training data\n",
        "# model_ngram.fit(train.data, train.target)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "# predictions = model_ngram.predict(test.data)\n",
        "# print(classification_report(test.target, predictions, target_names=train.target_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e7tdeNAQm0V"
      },
      "source": [
        "### Part 2: Comparing Different Naive Bayes Models\n",
        "\n",
        "Different types of Naive Bayes classifiers are suitable for different types of data. You will now explore how Bernoulli Naive Bayes, which is designed for binary/boolean features, performs on our text data compared to Multinomial Naive Bayes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAsd-vn-QsN4"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for the classifier and vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# TODO: Create a pipeline with TfidfVectorizer set to binary mode and BernoulliNB\n",
        "# TODO: Train and evaluate the BernoulliNB model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY7ZV181Qwpz"
      },
      "source": [
        "### Part 3: Hyperparameter Tuning\n",
        "\n",
        "Tuning model parameters can significantly improve performance. You will use grid search to experiment with different values of the `alpha` parameter in MultinomialNB, which controls smoothing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clx4shGPQzph"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for grid search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Setup the pipeline again with the standard TfidfVectorizer and MultinomialNB\n",
        "pipeline = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "\n",
        "# TODO: Define the parameter grid for 'alpha' in MultinomialNB\n",
        "# TODO: Initialize and fit a GridSearchCV object to find the best 'alpha'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWWagl-DQ-6K"
      },
      "source": [
        "### Conclusion and Discussion\n",
        "\n",
        "In this exercise, you engaged in advanced text classification tasks, exploring different feature engineering techniques with N-Grams, comparing various Naive Bayes classifiers, and tuning hyperparameters to optimize performance. These activities are essential for developing practical skills in machine learning and understanding the complexities and nuances of model optimization and evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn20M1qLsU4g"
      },
      "source": [
        "## **Exercise** 4: IMDB Dataset of 50K Movie Reviews\n",
        "\n",
        "In this exercise, your goal will be to use a Bayesian classifier to perform sentiment analysis on the IMDB Dataset of 50K Movie Reviews : https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "AHRUW6mLI7hA",
        "6g5KfVpSMBTL",
        "sfKut_n7MDGM",
        "RMcEwuvQMD2k",
        "Zx21tASgMEtJ",
        "92pg-21nMFjm",
        "zQVls1FEMfvq",
        "VU2B8CkfOadz",
        "4CjlQDSCOdG9",
        "vnGnnBUJNRjP",
        "8I1Fho6-Of_d",
        "qPqFYXKqRV62",
        "q3Plqa7JRX4k"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
