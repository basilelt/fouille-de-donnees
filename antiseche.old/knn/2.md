# k-Nearest Neighbors (k-NN)

k-Nearest Neighbors (k-NN) is an instance-based learning algorithm applicable for both classification and regression tasks. It operates as a lazy learning method, meaning no explicit model is built during training; computations occur at the time of prediction. Its effectiveness relies on calculating distances between data points, making preprocessing steps like feature scaling and appropriate handling of categorical features crucial. Challenges include computational cost for large datasets and sensitivity to the choice of 'K' and noisy data.

## Definition and Overview

- **Definition**: An instance-based learning algorithm for classification and regression.
- **How it Works**: Classifies new data points based on the majority class (or average for regression) of its K nearest neighbors in the training data.
- **Distance Metrics**: Uses metrics like Euclidean, Manhattan, and Minkowski to measure closeness.
- **Lazy Learning**: No model is built during training; all computations occur during prediction.
- **Use Cases**: Applied in image recognition, recommender systems, and pattern recognition.

## Historical Background

- **Origins**: Developed in the 1950s, rooted in pattern recognition theory.
- **Formalized**: By Thomas Cover and Peter Hart in 1967.
- **Development**: Enhanced with weighted voting and various distance metrics.
- **Modern Usage**: Applied in computer vision, recommender systems, healthcare, and available in many ML libraries.
- **Challenges and Evolution**: Faces computational complexity with large datasets; ongoing research focuses on optimization and parallelization.

## Advantages and Disadvantages

### Advantages
- **Simplicity**: Easy to understand and implement.
- **Versatility**: Can be used for both classification and regression.
- **No Model Training**: Lazy learning approach.
- **Non-Parametric**: Makes no assumptions about underlying data distribution.

### Disadvantages
- **Computational Cost**: Expensive in terms of time and memory, especially with large datasets.
- **Sensitive to Noisy Data**: Outliers can negatively impact performance.
- **Choice of 'K'**: Selecting an optimal value for 'K' can be challenging.
- **Scaling Required**: Requires feature scaling to prevent distance bias.

## Understanding the k-NN Algorithm

### Pseudo Code: 1-Nearest Neighbor (1-NN)

This is a simplified version for k=1.

```python
function 1NN(X_train, Y_train, x):
    closestDistance = infinity
    closestClass = null
    # Compute distances between x and all samples in X_train
    for i = 1 to length(X_train):
        dist = distance(x, X_train[i])
        if dist < closestDistance:
            closestDistance = dist
            closestClass = Y_train[i]
    return closestClass
```

- **Input**: Training data (X_train, Y_train), test sample x.
- **Output**: Predicted class for the test sample x.
- **Distance**: A distance function (e.g., Euclidean, Manhattan).

## Distance Metrics

- **Euclidean Distance**: Commonly used, computed as the square root of the sum of squared differences.  
  $d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$
- **Manhattan Distance**: Sum of the absolute differences between corresponding elements.  
  $d(x, y) = \sum_{i=1}^{n} |x_i - y_i|$
- **Minkowski Distance**: Generalization of Euclidean (p=2) and Manhattan (p=1) distances.  
  $d(x, y) = \left( \sum_{i=1}^{n} |x_i - y_i|^p \right)^{1/p}$
- **Cosine Similarity**: Measures the cosine of the angle between two vectors, often used with text data.  
  $\text{similarity}(x, y) = \frac{x \cdot y}{\|x\| \cdot \|y\|}$

## Choosing the Value of 'k'

The parameter K represents the "number of neighbors to inspect".

### Small Values of 'k'
- Sensitive to noise.
- Can lead to overfitting.
- Often results in a complex decision boundary.

### Large Values of 'k'
- More resistant to noise.
- Can lead to underfitting.
- Tends to produce a smoother decision boundary.

### Common Strategies
- **Cross-validation**: To test different 'k' values on the training set (do not test on the final test set).
- **Choosing an odd value for binary classification** to avoid ties.
- **Analyzing the error rate** for various 'k' values.

## Weighted vs. Unweighted Voting

### Unweighted Voting
- Every neighbor has an equal vote.
- Simple majority rule determines the class.

### Weighted Voting
- Neighbors' votes are weighted by their distance.
- Closer neighbors have a greater influence.
- **Example**: $w_i = \frac{1}{d(x, x_i)^2}$ (weight is inverse square of distance).
- **Comparison**: Weighted voting offers more nuanced predictions than unweighted voting.

## Preprocessing for k-NN

### Feature Scaling and Normalization
- **Feature Scaling**: Brings all features to the same scale, preventing features with larger scales from dominating distance computations.
- **Normalization (Min-Max Scaler)**: A special case of scaling that transforms the range of features to [0, 1].  
  $x_{\text{norm}} = \frac{x - x_{\min}}{x_{\max} - x_{\min}}$
- **Importance in KNN**: Ensures all features contribute equally to distance computation and prevents undue influence of features with larger numeric ranges.
- **Outliers**: Can significantly affect min-max scaling, potentially distorting the [0,1] range for the majority of data.

### Handling Categorical Features
- **Issue**: k-NN relies on distance computations, which are not directly applicable to categorical data.
- **Methods**:
  - **One-Hot Encoding**: Converts categories into binary (0 or 1) columns. This is suitable for nominal (unordered) categorical features.
  - **Ordinal Encoding**: Assigns ordered numbers to categories that inherently have an order (e.g., "un peu, beaucoup, Ã  la folie").
- **Distance Metrics**: Hamming Distance is suitable for binary representations (e.g., after one-hot encoding).
- **Challenges**:
  - One-Hot Encoding can increase dimensionality ("sparse matrix").
  - Ensuring meaningful distances between categories.

## Conclusion
- **Versatile Algorithm**: Suitable for classification, regression, and recommendation systems.
- **Simple Yet Powerful**: Effective with appropriate feature engineering.
- **Distance Metrics**: Choice of metric is crucial for performance.
- **Challenges**: Computationally intensive (especially with large datasets), may struggle in high-dimensional spaces. However, it is adaptable and can be updated with new data easily, making it suitable for "big data" if optimized.
- **Advanced Techniques**: Includes Kernelized k-NN, ensembles (e.g., combining k=1, k=2, k=3, similar to Mixture of Experts in LLMs), etc.
- **Practical Considerations**: Feature scaling, handling missing data, and selecting an optimal 'k' are vital.
- **Real-world Applications**: Used in medical diagnosis, financial forecasting, and more.