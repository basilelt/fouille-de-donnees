# Auto-generated viewer.py with embedded markdown content
# Generated by generate_viewer.py

EMBEDDED_CONTENT = {
    "bayes/1.md": """# Notes sur le Classifieur Bayesien Naïf

## Résumé

Le classifieur bayésien utilise les probabilités, notamment le théorème de Bayes et l'hypothèse d'indépendance des attributs, pour attribuer une classe à une instance donnée. Il vise à trouver l'hypothèse (classe) la plus probable en se basant sur les observations du jeu d'entraînement, ce qui le rend utile pour diverses tâches de classification, y compris l'analyse de texte.

## 1. Introduction au Classifieur Bayesien

- **Objectif:** Utiliser les probabilités pour la tâche de classification.
- **Principe:** Affecter à chaque hypothèse (classe) une probabilité d'être la bonne solution.
- **Méthode:** Observer des instances d'entraînement pour modifier les distributions de probabilité.
- **Tâche:** Trouver l'hypothèse la plus probable (la classe la plus probable) étant donnée une instance.
- **Base théorique:** S'appuie sur les probabilités conditionnelles et le théorème de Bayes.
- **Particularité:** Fait une hypothèse d'indépendance des attributs pour simplifier les calculs.

## 2. Rappels de Probabilités

- Probabilité d'un événement (PA): Entre 0 et 1.
- Événement certain: P = 1.
- Événement impossible: P = 0.
- Indépendance de A et B: P(A ∩ B) = P(A) * P(B)
- Probabilité de non-A: P(¬A) = 1 - P(A)
- Probabilité Conditionnelle (P(A sachant B)): Probabilité que A apparaisse, sachant que B est apparu.
- P(A | B) = P(A ∩ B) / P(B)
- D'où P(A ∩ B) = P(A | B) * P(B)
- Également P(A ∩ B) = P(B | A) * P(A)
- Événements Indépendants: Si A et B sont indépendants, P(A | B) = P(A). La connaissance de B ne modifie pas la probabilité de A.

## 3. Théorème de Bayes

- Formule: P(A | B) = [P(B | A) * P(A)] / P(B)

## 4. Application à la Classification (Fouille de Données)

- But: Calculer pour chaque classe C_k la probabilité qu'elle soit la solution sachant une instance X (description d'objet).
- Exemple: Pour un iris, P(setosa | pétal_length, sépal_length, ...)
- Formule appliquée: P(Classe | Description) = [P(Description | Classe) * P(Classe)] / P(Description)
- P(Classe | Description): Probabilité à postériori (ce que l'on cherche).
- P(Description | Classe): Probabilité de la description sachant la classe (vraisemblance).
- P(Classe): Probabilité a priori de la classe.
- P(Description): Probabilité de l'instance observée.
- Estimation des probabilités à partir du jeu d'apprentissage:
  - P(Classe): Proportion d'instances de cette classe dans le jeu d'entraînement.
  - Ex: Pour 50 Setosa sur 150 iris, P(Setosa) = 50/150 = 1/3.
  - P(Description): Proportion d'instances ayant ces valeurs d'attributs.
  - P(Description | Classe): Nombre de fois où cette description est constatée dans la classe C_k / Nombre total d'instances de la classe C_k.

### 4.1 Observations sur la Formule

- P(Classe | Description) augmente si P(Classe) augmente (classe plus fréquente).
- P(Classe | Description) augmente si P(Description | Classe) augmente (description fréquente dans cette classe).
- P(Classe | Description) diminue si P(Description) augmente (description est très courante toutes classes confondues, donc peu informative).

### 4.2 Maximum A Posteriori (MAP)

- On calcule P(Classe_k | Description) pour chaque classe C_k.
- On choisit la classe C_k avec la probabilité à postériori la plus élevée (arg max).
- Simplification: Le dénominateur P(Description) est une constante pour toutes les classes car il ne dépend pas de C_k.
- On peut donc le retirer pour trouver le maximum:
- argmax_k P(Classe_k | Description) = argmax_k [P(Description | Classe_k) * P(Classe_k)]

## 5. Hypothèse du Bayesien Naïf: Indépendance des Attributs

- Hypothèse: Les attributs sont indépendants les uns des autres sachant la classe.
- Conséquence: P(Description | Classe) = P(a_1, a_2, ..., a_n | Classe) peut être décomposé en un produit de probabilités:
- P(Description | Classe) = P(a_1 | Classe) * P(a_2 | Classe) * ... * P(a_n | Classe)
- Formule finale pour le classifieur Bayesien Naïf:
- argmax_k [P(Classe_k) * ∏_{i=1 à n} P(a_i | Classe_k)]
- Estimation pour attributs discrets:
- P(a_i | Classe_k) = (Nombre d'instances de Classe_k ayant la valeur a_i pour l'attribut i) / (Nombre total d'instances dans Classe_k).

## 6. Exemple 1: Prédiction de popularité d'un jeu

- Jeu d'entraînement: Jeux avec Genre (RPG/Action), Plateforme (Console/PC/Mobile), Budget Marketing (Faible/Moyen/Haut), et Populaire (Oui/Non).
- Nouvelle instance à classer (X): Genre=RPG, Plateforme=PC, Budget Marketing=Medium.
- Tâche: Prédire si le jeu sera Populaire=Oui ou Populaire=Non.

### 6.1 Calculs pour Populaire=Oui (Yes)

- P(Yes): Proportion de jeux "Oui" dans le jeu d'entraînement.
- Si 3 "Oui" sur 5 jeux: P(Yes) = 3/5.
- P(RPG | Yes): Proportion de jeux "Oui" qui sont RPG.
- Ex: 2/3.
- P(PC | Yes): Proportion de jeux "Oui" qui sont sur PC.
- Ex: 1/3.
- P(Medium | Yes): Proportion de jeux "Oui" avec budget Medium.
- Ex: 1/3.
- Calcul: P(Yes | X) = P(Yes) * P(RPG | Yes) * P(PC | Yes) * P(Medium | Yes)
- P(Yes | X) = (3/5) * (2/3) * (1/3) * (1/3) = 0.0444 (valeur non normalisée)

### 6.2 Calculs pour Populaire=Non (No)

- P(No): Proportion de jeux "Non".
- Si 2 "Non" sur 5 jeux: P(No) = 2/5.
- P(RPG | No): Proportion de jeux "Non" qui sont RPG.
- Ex: 1/2.
- P(PC | No): Proportion de jeux "Non" qui sont sur PC.
- Ex: 1/2.
- P(Medium | No): Proportion de jeux "Non" avec budget Medium.
- Ex: 1/2.
- Calcul: P(No | X) = P(No) * P(RPG | No) * P(PC | No) * P(Medium | No)
- P(No | X) = (2/5) * (1/2) * (1/2) * (1/2) = 0.05 (valeur non normalisée)

### 6.3 Prédiction

- P(No | X) (0.05) > P(Yes | X) (0.0444).
- Prédiction: Le jeu ne sera pas populaire.

### 6.4 Normalisation des Probabilités

- Les probabilités obtenues par produit sont souvent très petites.
- Méthode: Diviser chaque probabilité par la somme de toutes les probabilités des classes.
- Somme: 0.0444 + 0.05 = 0.0944
- P_norm(Yes | X) = 0.0444 / 0.0944 = 0.47 (47%)
- P_norm(No | X) = 0.05 / 0.0944 = 0.53 (53%)
- Avantage: Donne une distribution de probabilité sommée à 1, plus interprétable et donne une notion de certitude (ex: 53% vs 90%).

## 7. Gestion des Très Petites Probabilités

- Avec un grand nombre d'attributs (ex: 100), le produit de probabilités inférieures à 1 peut donner des valeurs extrêmement petites.
- Solution: Travailler dans un espace logarithmique.
- Transformation: log(a * b) = log(a) + log(b).
- Permet de transformer un produit en une somme, évitant les sous-dépassements numériques et les valeurs trop petites.

## 8. Gestion des Valeurs Numériques

- Problème: Impossible de calculer P(valeur_numérique | Classe) pour chaque valeur exacte (température, taille, etc.) car trop de valeurs possibles, ou trop peu d'instances pour chaque valeur.
- Solution: Estimer une distribution de probabilité pour les attributs numériques.
- Le plus simple: faire l'hypothèse d'une distribution Gaussienne (normale).
- On estime la moyenne (μ) et l'écart-type (σ) de l'attribut numérique pour chaque classe.
- La fonction de densité de probabilité gaussienne est ensuite utilisée pour estimer P(valeur_numérique | Classe).

## 9. Applications en Analyse de Texte

- Très utilisé pour la classification de texte (ex: détection de spam, attribution d'auteur, classification thématique).
- Défi: Représenter le texte non structuré en format attribut-valeur.
- Approche 1 (Naive): Chaque mot à chaque position est un attribut.
- Problème: Trop d'attributs et de valeurs de probabilité à estimer.
- Approche 2 (Bag-of-Words): S'abstraire de la position des mots.
- Représentation: Chaque mot du vocabulaire est un attribut. La valeur de l'attribut est soit sa présence/absence, soit son nombre d'occurrences (fréquence) dans le texte.
- Permet d'estimer les probabilités (P(mot | Classe)) en calculant les fréquences des mots dans les textes de chaque classe.

## 10. Avantages du Classifieur Bayesien Naïf

- Facile à implémenter.
- Relativement efficace.
- Interprétable: Produit des distributions de probabilité qui donnent une confiance dans la classification.
- Performant avec de petits jeux de données.
- Passe bien à l'échelle: Peut gérer un très grand nombre de caractéristiques.

## 11. Inconvénients et Limitations

- Hypothèse d'indépendance des attributs: Rarement vraie dans la réalité.
- Peut être modélisée, mais rend l'algorithme plus complexe et coûteux.
- Problème des Zéro Probabilités:
  - Si une valeur d'attribut n'apparaît jamais pour une certaine classe, P(attribut | Classe) sera 0.
  - Étant donné que toutes les probabilités sont multipliées, cela annule le score de la classe, même si d'autres attributs suggèrent fortement cette classe.
  - Solution pratique: Utiliser un petit epsilon (ε) ou une technique de lissage (ex: lissage de Laplace) au lieu de 0 pour les probabilités nulles.
- Modèles complexes: Moins adapté aux modèles avec beaucoup d'interactions complexes entre les caractéristiques.
- Des extensions existent pour modéliser ces interactions, mais sont plus coûteuses.

Le Bayesien Naïf est une base solide et performante pour des problématiques simples, avec de nombreuses évolutions pour des cas plus complexes.""",
    "clustering/1.md": """# Notes sur le Clustering Hiérarchique

Le clustering hiérarchique est une tâche d'apprentissage non supervisé qui vise à construire des groupes (clusters) d'objets similaires, représentés par une structure arborescente (dendrogramme). Il se base sur des mesures de dissimilarité entre objets et la manière de calculer la distance entre les clusters, ce qui impacte fortement les résultats.

## 1. Introduction au Clustering

- **Définition**: Le clustering est une tâche non supervisée, contrairement à la classification qui est supervisée et utilise des données labellisées.
- **Objectif**: Construire des groupes d'objets (clusters) à partir de leur description, sans labels préexistants.
- **Principes**:
  - Maximiser la similarité intracluster (objets du même groupe doivent être les plus proches possible).
  - Minimiser la similarité intercluster (objets de groupes différents doivent être les plus éloignés possible).

## 2. Mesure de Dissimilarité (Distance)

- **Concept**: Définir une mesure pour évaluer de manière graduelle le degré de ressemblance entre deux objets.
- **Propriétés**: Pour être appelée "distance", cette mesure doit satisfaire certaines propriétés mathématiques.
- **Exemple (Clients)**: Marie, Bruno, Laurent décrits par l'âge et le salaire.
- **Visualisation dans un espace à deux dimensions**.
- **Distance Euclidienne**:
  - Souvent utilisée pour des vecteurs de valeurs numériques.
  - Calculée comme la racine carrée de la somme des différences au carré des caractéristiques.
- **Normalisation des Caractéristiques**:
  - Nécessaire avant le calcul de la distance (en particulier Euclidienne) pour éviter qu'une caractéristique avec une plus grande amplitude ne domine le calcul.
  - Objectif: Assurer que chaque caractéristique ait le même poids dans le calcul de la distance.
  - **Méthode: Normalisation Min-Max**:
    - Formule: $(valeur - min) / (max - min)$
    - Ramène toutes les caractéristiques dans le même domaine de définition (généralement entre 0 et 1).
- **Matrice de Distance**:
  - Matrice 2x2 qui stocke la distance entre chaque couple d'objets.
  - Complexité quadratique par rapport au nombre d'objets.
- **Choix de la Distance**: Dépend du type de données (ex: numérique, images, vidéos, sons, documents, utilisateurs). Il est crucial de définir une distance pertinente pour les objets à regrouper.

## 3. Algorithme de Clustering Hiérarchique Ascendant (Agglomératif)

- **Processus**:
  1. **Initialisation**: Chaque objet est un cluster unique (ex: 100 objets = 100 clusters).
  2. **Itération**:
     - Calculer la matrice de distance 2 à 2 entre tous les clusters.
     - Sélectionner les deux clusters les plus proches (distance minimale).
     - Fusionner ces deux clusters en un seul.
  3. **Terminaison**: Répéter jusqu'à ce que tous les objets soient dans un seul groupe (un seul cluster).
- **Dendrogramme**:
  - Représentation hiérarchique des fusions successives.
  - La hauteur des liens est proportionnelle à la distance à laquelle les clusters ont été fusionnés.
  - **Découpe**: Permet d'obtenir une partition en un nombre désiré de clusters en "coupant" le dendrogramme à une certaine hauteur. La forme du dendrogramme peut donner une indication sur le nombre de clusters.
- **Critère de Lien (Linkage) - Mesure de la distance entre groupes**: C'est un paramètre clé qui définit comment la distance entre deux clusters est calculée.
  - **Distance Minimum (Single Linkage)**: Distance entre les deux objets les plus proches de chaque cluster. Tendance à l'effet de "chaînage".
  - **Distance Maximum (Complete Linkage)**: Distance entre les deux objets les plus éloignés de chaque cluster.
  - **Distance Moyenne (Average Linkage)**: Moyenne de toutes les distances possibles entre paires d'objets des deux clusters. Coûteux.
  - **Centroïde**: Distance entre les centroïdes (moyennes) des clusters.
  - **Critère de Ward**: Basé sur l'inertie, tend à créer des clusters de cardinalités (nombre d'objets) similaires.
- Le choix du critère de lien entraîne des résultats de clustering différents même avec les mêmes données.

## 4. Exemples d'Utilisation

- **Contrainte**: La complexité quadratique de la matrice de distance limite son application aux grands volumes de données. Utilisé quand le nombre d'objets est "acceptable".
- **Applications**:
  - Bioinformatique: Regroupement de données d'expression de gènes.
  - Neurosciences: Clustering de représentations d'images issues d'IRM cérébrales pour étudier la représentation abstraite et visuelle.
  - Analyse comportementale: Clustering de séquences d'activités de chirurgiens (seniors vs. juniors) pour identifier des schémas opératoires.

## 5. Évaluation du Clustering

- **Différence avec la classification**: Pas toujours de vérité terrain (labels de classe) pour vérifier la qualité des clusters.
- **Deux approches principales**:
  - **Critères Internes**: Basés uniquement sur la qualité intrinsèque des clusters.
    - Mesurent la compacité (objets proches au sein d'un cluster) et la séparation (clusters distincts les uns des autres) dans l'espace des données.
    - Exemples: Coefficient de Silhouette, Dunn Index.
  - **Critères Externes**: Comparent la partition obtenue par l'algorithme à un clustering ou à des classes préexistantes (si disponibles).
    - Gèrent l'absence de correspondance directe entre les numéros de cluster (ex: Cluster 1) et les noms de classes (ex: Iris setosa), ainsi que la possibilité d'un nombre de clusters différent.
    - Exemples: Rand Index, Adjusted Rand Index.

## 6. Conclusion : Avantages et Inconvénients du Clustering Hiérarchique Ascendant

- **Avantages**:
  - **Simplicité et Intuitivité**: Facile à comprendre (processus itératif de regroupement).
  - **Flexibilité du nombre de classes**: Le nombre de clusters n'est pas un paramètre initial; il est choisi après la construction du dendrogramme par une simple coupe.
  - **Visualisation intuitive**: Le dendrogramme aide à orienter le choix du nombre de clusters.
- **Inconvénients**:
  - **Coût de la matrice de distance**: La complexité quadratique $O(n^2)$ (où n est le nombre d'objets) rend l'algorithme inadapté aux très grands volumes de données.
  - **Décisions irréversibles**: Les fusions de clusters ne sont jamais remises en question au cours de l'algorithme (approche gloutonne).
  - **Variabilité**: Les résultats dépendent fortement du critère de lien choisi (single, complete, average, Ward, etc.), ce qui peut rendre difficile le choix du "meilleur" critère.""",
    "clustering/2.md": """# Data Mining - Hierarchical Clustering

**Presented by Germain Forestier, PhD, Université de Haute-Alsace**

## Summary/Main Takeaway

Hierarchical clustering is an unsupervised data mining technique that organizes a set of objects into a hierarchy of clusters, often visualized as a dendrogram. It involves iteratively merging or splitting clusters based on defined dissimilarity measures and linkage criteria. While it provides intuitive visualization and flexibility in determining the number of clusters, it can be computationally intensive and results may vary depending on the chosen distance and linkage methods.

## 1. Introduction to Clustering

### Objectives:
- Divide a set of objects into groups (clusters).
- Maximize intra-cluster similarity and minimize inter-cluster similarity.
- Define a measure of dissimilarity (or distance) between objects.
- Clustering is an unsupervised task.

### Dissimilarity between Objects:
- Allows gradual evaluation of resemblance.
- Must satisfy mathematical properties to be called a distance.

#### For numerical values:
- **Euclidean distance:** $\\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$
- Requires scaling of attributes (e.g., Age vs. Salary).

**Scaling techniques:**
- **MinMax scaling:** $z = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}$
- **Example:** Comparing clients (Marie, Bruno, Laurent) using scaled Age and Salary. $d(\\text{Marie}, \\text{Bruno}) \\approx 1.22$.

- Other distances exist for numerical values.

#### For other types of data:
- Specific distances for each data type.
- Objects can be described by multiple data types (e.g., image, video, sound, document, user).

## 2. Hierarchical Clustering Method

### Method Steps:
1. Start with one cluster per object.
2. At each step, two clusters are merged.
3. Cut the tree to obtain a partition.
4. Calculate the pairwise distance matrix between objects.
5. Select the smallest value in this matrix to identify the most similar groups.
6. Merge these two groups, replacing their rows with a row for the newly created group (based on a chosen criterion).
7. Repeat until a single group containing all objects is obtained.
8. This creates a sequence of partitions where the number $k$ of groups varies from $m$ (number of objects) to 1.
9. Finally, choose the value of $k$ using various possible methods.

### Dendrogram:
- A hierarchical representation of successive merges.
- The height of a link is proportional to the distance at which clusters were merged.

### Defining the Distance Between Clusters (Linkage Criteria):
1. **Minimum jump (single linkage):** Sensitive to noise.
2. **Maximum jump (complete linkage):** Tends to produce specific clusters; sensitive to noisy individuals.
3. **Average jump (average linkage):** Tends to produce clusters with similar variance.
4. **Centroid:** Good resistance to noise; not always possible to calculate.
5. **Ward:** Aggregation criterion based on inertia; creates "balanced" clusters.

**Note:** Different linkage criteria can lead to significantly different clustering results.

## 3. Example of Usage
- Clustering of gene expression data (Eisen et al., 1998).
- Clustering of similar image representation (Mur et al., 2013).
- Study of surgeon behavior (Forestier et al., 2012).

## 4. Clustering Evaluation

### Internal Evaluation:
- Evaluates the quality of clusters based solely on the data itself, without external labels.
- Focuses on compactness and separation of clusters.

#### Examples of Criteria:
- **Silhouette Coefficient:** $s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}$
  - $a(i)$: average distance between point $i$ and all other points in the same cluster.
  - $b(i)$: minimum average distance between point $i$ and points in another cluster.

- **Dunn Index:** $D = \\frac{\\min_{1 \\leq i < j \\leq k} \\delta(C_i, C_j)}{\\max_{1 \\leq l \\leq k} \\Delta(C_l)}$
  - $\\delta(C_i, C_j)$: distance between clusters $C_i$ and $C_j$.
  - $\\Delta(C_l)$: diameter of cluster $C_l$.

### External Evaluation:
- Compares clustering results to a ground truth or external labels.
- Measures how well the clustering matches the true classification.

#### Examples of Criteria:
- **Rand Index (RI):** $RI = \\frac{TP + TN}{TP + FP + FN + TN}$
  - $TP$: True Positives; $TN$: True Negatives.
  - $FP$: False Positives; $FN$: False Negatives.

- **Adjusted Rand Index (ARI):** $ARI = \\frac{RI - E[RI]}{\\max(RI) - E[RI]}$
  - $E[RI]$: Expected Rand Index of random clustering.

## 5. Conclusion

### Advantages:
- Easy to understand.
- Allows easy variation in the number of clusters.
- Intuitive visualization (dendrogram).

### Disadvantages:
- Computational cost of the distance matrix.
- The clustering criterion depends on the groups already formed.
- Different results depending on the clustering criterion (linkage method).""",
    "decision-tree/1.md": """# Notes sur les Arbres de Décision

Les arbres de décision sont une famille d'algorithmes de fouille de données très utilisés et interprétables, permettant la classification (prédiction de valeurs discrètes) et la régression (prédiction de valeurs continues). Leur nature visuelle et la facilité d'explication de leurs décisions en font un outil précieux, notamment pour la communication avec des experts non-informaticiens.

## 1. Qu'est-ce qu'un Arbre de Décision ?

- **Définition**: Une structure d'arbre où chaque composant a une fonction spécifique.
- **Structure**:
  - **Nœuds**: Représentent des tests sur les attributs du jeu de données.
  - **Branches**: Correspondent aux valeurs possibles que peut prendre un attribut.
  - **Feuilles**: Représentent une classe (pour la classification) ou une valeur continue (pour la régression).

## 2. Jeu de Données Exemple: Météo et Jeu

- **Contexte**: Prédiction de la participation à un jeu (ex: golf/tennis) en fonction des conditions météorologiques.
- **Attributs (exemples)**:
  - État du ciel (Sky)
  - Température (Temperature)
  - Humidité (Humidity)
  - Vent (Wind)
- **Classe à prédire**: Jouer (Yes/No).
- **Processus d'inférence**: Parcourir l'arbre en fonction des conditions météo pour arriver à une feuille (décision Yes/No).

## 3. Construction de l'Arbre de Décision: Algorithme ID3

- **Algorithme principal**: ID3 (Iterative Dichotomiser 3)
- **Proposé par**: Quinlan en 1986.
- **Approche**: top-down (descendante), itérative.
- **Concepts clés**:
  - **Entropie**: Mesure l'incertitude d'un ensemble de données.
    - Maximale quand il y a équiprobabilité (ex: 0.5 probabilité pour deux événements, entropie = 1).
    - Nulle quand il n'y a aucune incertitude (ex: 100% de "Yes").
  - **Gain d'information**: Mesure la réduction de l'incertitude sur la classification suite à la connaissance d'un attribut.
    - L'objectif est de trouver l'attribut qui offre le plus grand gain d'information à chaque étape.
- **Processus de construction (simplifié)**:
  1. Calculer l'entropie de la classe de la population totale (entropie initiale).
  2. Pour chaque attribut, calculer le gain d'information qu'il apporte.
  3. Choisir l'attribut avec le gain d'information le plus élevé comme nœud racine (ou nœud interne).
  4. Répéter le processus récursivement pour chaque branche, jusqu'à ce que les feuilles soient pures (toutes de la même classe) ou qu'un critère d'arrêt soit atteint.
- **Exemple de calcul initial** (9 Yes, 5 No): Entropie de la population ≈ 0.94.

## 4. Gestion des Valeurs Numériques

- **Problème**: Impossible de créer une branche pour chaque valeur continue possible (ex: température en degrés Fahrenheit, humidité en pourcentage).
- **Solution**: Discrétisation
  - Transformer une variable continue en variable discrète (ex: température < 70 = "froid", 70-75 = "doux", > 75 = "chaud").
  - Peut être effectuée en pré-traitement des données.
  - Certains algorithmes (ex: C4.5) intègrent la discrétisation et choisissent les seuils de coupe optimaux pendant la construction de l'arbre.

## 5. Surapprentissage (Overfitting)

- **Définition**: Le modèle adhère trop aux données d'entraînement, ce qui le rend incapable de bien généraliser sur de nouvelles données.
- **Causes spécifiques aux arbres de décision**:
  - Profondeur excessive de l'arbre.
  - Trop de petites coupes spécialisées sur les données d'entraînement.
  - Données d'entraînement insuffisantes.
  - Absence d'élagage (pruning).
- **Stratégies d'Élagage (Pruning) pour l'éviter**:
  - Supprimer les nœuds qui n'améliorent pas significativement les performances.
  - Ajouter des scores de pénalité pour les arbres trop complexes.
  - Utiliser la théorie MDL (Minimum Description Length) pour trouver la description minimale optimale.
  - Techniques d'élagage basées sur les erreurs.
  - Contrôler la profondeur maximale de l'arbre.
  - Pré-élagage (contraintes pendant la construction) vs. Post-élagage (modification de l'arbre après construction).

## 6. Forêts Aléatoires (Random Forests)

- **Concept**: Une collection d'arbres de décision (ensemble learning).
- **L'idée est de construire plusieurs arbres et de combiner leurs décisions/prédictions pour obtenir une prédiction finale.**
- **Avantages**:
  - Très performantes et largement utilisées.
  - Peuvent surpasser l'apprentissage profond sur certaines applications spécifiques.
- **Diversité des arbres**: Essentielle car la construction d'un arbre unique (ex: avec ID3) est déterministe.
- **Méthodes pour introduire la diversité**:
  - Utiliser différents algorithmes de construction.
  - Appliquer différentes méthodes d'élagage.
  - **Échantillonnage des données**:
    - Échantillonnage des instances (ne pas utiliser toutes les données pour chaque arbre).
    - Échantillonnage des attributs (ne pas utiliser tous les attributs pour chaque arbre).

## 7. Avantages et Inconvénients des Arbres de Décision

- **Avantages**:
  - Interprétables et explicables (très visuels).
  - Faible préparation des données (normalisation souvent intégrée).
  - Gestion des données numériques et catégorielles.
  - Capacité à représenter des frontières de classe non linéaires.
- **Désavantages**:
  - Surapprentissage fréquent (sans élagage).
  - Instabilité: Les petits changements dans les données peuvent entraîner des modifications significatives de l'arbre.
  - Complexité de mise à jour: Nécessite souvent une reconstruction complète de l'arbre lors de l'ajout de nouvelles données.
  - Biais: Peut survaloriser des attributs avec de nombreuses valeurs possibles.
  - Optimum local: L'algorithme glouton de construction ne garantit pas l'optimum global de l'arbre.

## 8. Conclusion

Les arbres de décision, bien que potentiellement instables et sujets au surapprentissage, restent un modèle très pertinent, surtout pour les petits volumes de données. Les Random Forests, leurs dérivés, sont des algorithmes extrêmement performants et très utilisés dans diverses applications.""",
    "decision-tree/2.md": """# Decision Trees: Concepts, Construction, and Overfitting

Decision Trees are intuitive, flowchart-like models used for classification and regression tasks, easily visualized and interpreted. They are built by recursively splitting data based on attributes to reduce uncertainty, but can be prone to overfitting.

## 1. Introduction and Motivation

- **Definition**: A flowchart-like tree structure where:
  - Each internal node denotes a test on an attribute.
  - Each branch represents an outcome of the test.
  - Each leaf node holds a class label or continuous value (decision).
- **Purpose**: Used for both classification and regression.
- **Structure**:
  - Built from data.
  - Nodes: attributes.
  - Branches: values.
  - Leaves: decisions (classes).
- **Classification Process**: A new instance is tested by its path from the root to a leaf node.

## 2. ID3 (Iterative Dichotomiser 3)

- **Developer**: Ross Quinlan in 1986.
- **Purpose**: Create a decision tree to classify instances.
- **Approach**:
  - Top-down, greedy search through possible branches (no backtracking).
  - Selects the attribute that is most informative (highest information gain) as the decision node, proceeding recursively.
- **Attribute Selection**: Based on entropy and information gain.
- **Limitations**:
  - Can overfit the data.
  - Biased towards attributes with many outcomes.
  - Does not handle numeric attributes or missing values directly.

## 3. Criteria for Splitting

### Entropy Heuristic

- **Measures**: The amount of information or uncertainty in a set of examples (E).
- **Formula**:
  $$
  H(E) = -\\sum (p_i \\cdot \\log_2(p_i))
  $$
  Where \\( p_i \\) is the proportion of examples of class \\( i \\) in set \\( E \\).
- **Interpretation**:
  - Maximal Uncertainty: If classes are equally distributed (e.g., 4 yes, 4 no out of 8), \\( H(E) = 1 \\).
  - Zero Uncertainty: If all examples belong to one class (e.g., 8 yes out of 8), \\( H(E) = 0 \\).
  - Higher entropy implies greater uncertainty; lower entropy implies less uncertainty.

### Entropy Gain (Information Gain)

- **Measures**: Reduction in entropy (uncertainty) caused by partitioning a set of examples according to an attribute.
- **Formula**:
  $$
  G(a, E) = H(E) - \\sum \\left( \\frac{|E_{a,v}|}{|E|} \\cdot H(E_{a,v}) \\right)
  $$
  Where:
  - \\( E \\) is the set of examples.
  - \\( a \\) is the attribute.
  - \\( V(a) \\) is the set of values for attribute \\( a \\).
  - \\( E_{a,v} \\) is the subset of \\( E \\) where attribute \\( a \\) has value \\( v \\).
- **Objective**: The ID3 algorithm selects the attribute with the highest information gain to split the node.
- **Example Calculations**:
  - Initial population: 9 Yes, 5 No.
  - \\( H(E) \\approx 0.94 \\)
  - **Attribute Sky**:
    - Sunny (2 Yes, 3 No): \\( H(\\text{sunny}) \\approx 0.97 \\)
    - Overcast (4 Yes, 0 No): \\( H(\\text{overcast}) = 0 \\)
    - Rain (3 Yes, 2 No): \\( H(\\text{rain}) \\approx 0.97 \\)
    - \\( G(\\text{sky}, E) \\approx 0.94 - \\left( \\frac{5}{14} \\cdot 0.97 + \\frac{4}{14} \\cdot 0 + \\frac{5}{14} \\cdot 0.97 \\right) \\approx 0.246 \\)
  - **Attribute Humidity**:
    - High (3 Yes, 4 No): \\( H(\\text{high}) \\approx 0.98 \\)
    - Normal (6 Yes, 1 No): \\( H(\\text{normal}) \\approx 0.59 \\)
    - \\( G(\\text{humidity}, E) \\approx 0.94 - \\left( \\frac{7}{14} \\cdot 0.98 + \\frac{7}{14} \\cdot 0.59 \\right) \\approx 0.151 \\)
  - **Attribute Wind**:
    - Strong (3 Yes, 3 No): \\( H(\\text{strong}) = 1 \\)
    - Weak (6 Yes, 2 No): \\( H(\\text{weak}) \\approx 0.81 \\)
    - \\( G(\\text{wind}, E) \\approx 0.94 - \\left( \\frac{6}{14} \\cdot 1 + \\frac{8}{14} \\cdot 0.81 \\right) \\approx 0.048 \\)
  - **Conclusion**: Sky has the highest gain (0.246), so it would be chosen as the root node.

## 4. Handling Numerical Data

- **Challenge**: Cannot calculate gain for each continuous value directly.
- **Solution**: Discretize the variable (e.g., < 70 = Cold, [70-75] = Mild, > 75 = Hot).
- **Thresholds**: Can be found before or during tree construction (e.g., C4.5 algorithm).

## 5. Overfitting in Decision Trees

- **Definition**: Occurs when a model learns the training data too well, including noise and outliers, leading to poor generalization on new data.
- **Causes**:
  - Depth: Trees allowed to grow too deep create overly specific rules.
  - Complexity: Small splits deep in the tree capture noise.
  - Insufficient Data: Limited data leads to patterns that don't generalize.
  - No Pruning: Lack of pruning mechanisms.

### Pruning Strategies to Avoid Overfitting

- **Pruning**: Reducing tree size by converting branch nodes into leaf nodes, improving generalization.
- **Main Strategies**:
  - **Reduced Error Pruning**: Remove a node if it doesn't decrease prediction accuracy on a validation set.
  - **Cost Complexity Pruning (CCP)**: Introduces a penalty for complexity (number of leaf nodes) to minimize misclassification rate + penalty.
  - **Minimum Description Length (MDL)**: Prune based on Occam's razor (simplest model that fits the data).
  - **Minimum Error Pruning**: Prune nodes whose removal would not significantly increase the error rate.

## 6. Random Forest

- **Concept**: Builds multiple decision trees using subsets of the data.
- **Mechanism**: Averages the predictions of individual trees for the final decision.
- **Benefits**: Helps to reduce overfitting and can assess attribute importance.

## 7. Advantages and Disadvantages of Decision Trees

### Advantages

- **Interpretable**: Easy to understand and visualize.
- **Minimal Data Prep**: No need for normalization or scaling.
- **Handle Multiple Types**: Can deal with numerical and categorical data.
- **Non-linear Relationships**: Naturally handles non-linearity.

### Disadvantages

- **Overfitting**: Can become too complex, capturing noise.
- **Instability**: Small data changes can lead to different tree structures.
- **Bias**: Biased towards features with more levels.
- **Locally Optimal**: Greedy algorithms might not find the globally optimal tree.""",
    "introduction.md": """# Synthèse des Notes sur le Data Mining

Le Data Mining est un processus interdisciplinaire qui consiste à découvrir des modèles, des relations et des connaissances à partir de grands ensembles de données en utilisant des algorithmes et des méthodes statistiques. Il s'inscrit dans un écosystème plus large incluant l'Intelligence Artificielle (IA), le Machine Learning (ML), et le Big Data, jouant un rôle crucial dans la prise de décision éclairée dans de nombreux secteurs. La qualité des données et la compréhension des objectifs métier sont primordiales pour le succès des projets de Data Mining.

# Data Mining - Introduction

## 1. Vue d'ensemble du Data Mining

### Définition
- Découvrir des modèles, relations, et connaissances à partir de grands ensembles de données.
- Appliquer des algorithmes et méthodes statistiques pour analyser et interpréter les données.
- Le terme est apparu dans les années 1990, mais ses origines sont plus anciennes.
- Combine des techniques de la statistique, de la reconnaissance de formes et du Machine Learning.
- Racines dans les mathématiques, l'informatique et la théorie de l'information.

### Contexte Historique de l'IA
- 1950-1960: Historiquement liée à la physique statistique.
- Aujourd'hui: IA est omniprésente grâce à l'accès à d'énormes quantités de données (internet, livres, etc.).

### Applications Industrielles
- Affaires et Finance: Comportement client, détection de fraude, gestion des risques, segmentation de marché.
- Santé et Médecine: Prédiction de maladies, soins aux patients, découverte de médicaments (ex: maladies rares).
- Ingénierie: Contrôle qualité, prédiction de pannes, optimisation des processus.
- Environnement et Agriculture: Prévisions météorologiques, rendements des cultures, gestion des ressources.

### Data Mining vs. Data Analysis vs. Machine Learning
- Data Analysis: Se concentre sur la compréhension des données et l'obtention d'informations via des méthodes statistiques.
- Data Mining: Va au-delà de l'analyse, utilisant des algorithmes pour trouver des modèles et prédire des tendances. Son périmètre inclut le prétraitement, l'analyse exploratoire et l'interprétation des résultats.
- Machine Learning (ML): Sous-ensemble de l'IA, entraîne des modèles à apprendre des données. Utilisé dans le Data Mining mais non synonyme.

### Intersections avec d'autres Disciplines
- Intelligence Artificielle (IA), Machine Learning, Big Data, Recherche d'Information, Reconnaissance de Formes.
- Text Mining, Gestion de Bases de Données, Analyse Prédictive, Traitement du Langage Naturel (NLP) (proche du TAL - Tout Analysis Language), Vision par Ordinateur (OCR).

### Relation AI, ML, Deep Learning
Le Deep Learning est un sous-ensemble du Machine Learning, qui est lui-même un sous-ensemble de l'Intelligence Artificielle.

### Domaine "Tendance"
Les termes comme "Machine Learning", "Big Data", "Data Science", "AI" sont souvent utilisés de manière interchangeable, parfois sur-médiatisés ou mal employés.

### Le "Data Scientist"
Profession très demandée nécessitant des compétences transdisciplinaires. (Aujourd'hui, il existe des profils comme AI engineer / AI scientist).

## 2. Types et Sources de Données

### Données Structurées vs. Non Structurées
- Données Structurées: Organisées dans un format ou un schéma défini (ex: tables, CSV). Facilement interrogeables.
  - Exemple: Jeu de données Iris (longueur/largeur sépale et pétale pour prédire l'espèce).
- Données Non Structurées: Manquent de forme spécifique (ex: documents texte, vidéos, images, posts réseaux sociaux). Nécessitent des techniques de traitement spécialisées.
- Problèmes: Classes mal balancées (ex: diagnostic de maladies rares où "pas malade" représente 99% de précision mais est inutile).

### Formats de Données Courants
- CSV (Comma-Separated Values): Texte brut, utilise des virgules pour séparer les valeurs.
- JSON (JavaScript Object Notation): Basé sur du texte, lisible par l'homme, pour échanger des données web.
- XML (eXtensible Markup Language): Langage de balisage, structure auto-descriptive, services web.
- Autres: YAML, HDF5.

### Sources de Données
- Bases de Données: Repositories structurés (relationnelles, documentaires, clé-valeur).
- Web Scraping: Extraction de données de sites web (peut être automatisée).
- APIs (Application Programming Interfaces): Protocole pour construire et interagir avec des applications logicielles, souvent pour récupérer des données web.

## 3. Prétraitement des Données

### Importance de Données Propres et de Qualité
- Données Propres: Sans erreurs ni incohérences, améliore la précision des modèles analytiques, facilite l'intégration.
- Données de Qualité: Respecte des normes (précision, complétude, fiabilité), soutient la prise de décision, impacte le succès des projets.
- Conséquences d'une mauvaise qualité: Conclusions incorrectes, augmentation des coûts opérationnels, diminution de la confiance.
- Notes de séance: La préparation des données prend beaucoup de temps. Les données ne sont pas toujours disponibles ou utilisables (nécessitent un cleaned dataset).

### Techniques de Nettoyage des Données
- Gestion des valeurs manquantes:
  - Supprimer les enregistrements.
  - Imputer les valeurs (moyenne, médiane, mode).
  - Utiliser des modèles ML pour prédire les valeurs manquantes.
- Transformation des données: Normalisation/mise à l'échelle, conversion de types/formats, encodage de variables catégorielles.
- Détection d'aberrations (Outliers): Identifier et gérer les valeurs anormales (méthodes statistiques ou ML). À faire avec prudence car elles peuvent contenir des informations importantes.
- Jeu d'entraînement et jeu de test: Les données aberrantes sont pertinentes ici.
- Validation et Vérification: S'assurer de la précision des données, vérifier avec des sources externes fiables.

## 4. Analyse Exploratoire des Données (EDA)

### But de l'EDA
- Comprendre la structure des données: Analyser la forme, la tendance centrale, la dispersion, visualiser les relations entre variables.
- Identifier les modèles et anomalies: Découvrir les tendances, clusters, outliers, erreurs potentielles.
- Faciliter la communication: Utiliser des visualisations pour rendre les données complexes compréhensibles.

### Techniques de Visualisation des Données
- Diagrammes à barres: Fréquence ou décompte par catégorie.
- Nuages de points: Relation entre deux variables continues.
- Graphiques linéaires: Tendances au fil du temps.
- Histogrammes: Distribution d'une variable continue.
- Boîtes à moustaches (Box Plots): Tendance centrale et variabilité.
- Considération: Choisir la technique appropriée en fonction du type de données et de la question.

## 5. Outils et Logiciels pour le Data Mining

### Outils Populaires
- Python: Langage généraliste, écosystème riche (ex: pandas, matplotlib, scikit-learn, NumPy).
- R: Langage pour le calcul statistique, vaste bibliothèque de packages.
- SQL: Langage de requête pour bases de données relationnelles (manipulation, agrégation, jointures).
- Excel: Logiciel de tableur, outils intégrés pour petites analyses.

### Bibliothèques et Packages
- Pandas: Manipulation et analyse de données, structures de données efficaces (dataframe).
- NumPy: Calcul numérique, grands tableaux et matrices.
- Matplotlib: Visualisation (statique, interactive, animée).
- Scikit-learn: Machine Learning (classification, régression, clustering, prétraitement).

## 6. Concepts de Base en Statistique et Machine Learning

### Statistiques Descriptives vs. Inférentielles
- Descriptives: Résument les aspects principaux d'un jeu de données (moyenne, médiane, mode, écart-type). Fournit un aperçu.
- Inférentielles: Fait des prédictions ou des inférences sur une population à partir d'un échantillon (tests d'hypothèses, intervalles de confiance, régression). Tire des conclusions générales.

### Apprentissage Supervisé vs. Non Supervisé
- Supervisé: Nécessite un jeu de données étiqueté (paires entrée-sortie). Le but est d'apprendre une fonction qui mappe les entrées aux sorties (ex: classification, régression). Utilise des données passées avec un label pour prédire le futur.
- Non Supervisé: Travaille avec des données non étiquetées. Le but est d'identifier des modèles, structures ou relations (ex: clustering, réduction de dimensionnalité, règles d'association). Utilisé quand il n'y a pas de label (ex: description). Algorithmes auto-supervisés (self-supervised) comme les LLMs créent leurs propres tâches (ex: masquer du texte dans une phrase).

### Compromis Biais-Variance (Bias-Variance Tradeoff)
- Biais: Erreur due à la simplification excessive du modèle. Un biais élevé entraîne un sous-apprentissage (underfitting), ne parvenant pas à capturer le modèle sous-jacent.
- Variance: Erreur due à la sensibilité aux petites fluctuations des données. Une variance élevée entraîne un sur-apprentissage (overfitting), capturant le bruit plutôt que le modèle.
- Compromis: Réduire le biais augmente la variance, et vice-versa. L'objectif est d'équilibrer les deux pour minimiser l'erreur totale. La précision n'est pas la seule métrique; l'interprétabilité est aussi cruciale.

## 7. Le Processus de Data Mining (CRISP-DM)

1. Compréhension de l'entreprise (Business Understanding): Définir les objectifs et les buts métier.
   - Importance: Assure l'alignement avec les stratégies, clarifie le problème, définit les métriques de performance, optimise l'allocation des ressources, facilite la communication, évalue l'impact, gère les risques.
2. Compréhension des données (Data Understanding): Collecter, décrire, explorer et vérifier la qualité des données.
3. Préparation des données (Data Preparation): Nettoyer, transformer, intégrer, sélectionner et formater les données.
4. Modélisation (Modeling): Sélectionner les techniques, concevoir les tests, construire et évaluer les modèles.
5. Évaluation (Evaluation): Évaluer la qualité du modèle, réviser le processus, déterminer les étapes suivantes.
6. Déploiement (Deployment): Planifier le déploiement, la maintenance et la surveillance du modèle.

## 8. Introduction au Big Data et à la Scalabilité

### Comprendre le Big Data
- Caractérisé par les "5 V":
  - Volume: Grandes tailles de données (téraoctets, pétaoctets).
  - Vélocité: Vitesse de génération et de traitement des données.
  - Variété: Types de données (structurées, semi-structurées, non structurées).
  - Véracité: Qualité et fiabilité des données.
  - Valeur: Valeur potentielle dérivée des données.
- Défis: Stockage, traitement, analyse, sécurité.
- Technologies: Hadoop, Spark, bases de données NoSQL.

### Défis liés au Big Data
- Stockage: Gestion des grands volumes, distribution, redondance.
- Traitement: Traitement efficace, souvent via le calcul parallèle.
- Intégration: Fusion de données de sources et formats divers.
- Qualité: Assurer la précision, la cohérence et la fiabilité.
- Sécurité: Protection de la vie privée, intégrité, conformité réglementaire.
- Analyse: Extraction d'insights à partir de jeux de données complexes et diversifiés.
- Scalabilité: Mise à l'échelle des systèmes pour gérer la croissance des données sans perte de performance.
- Coût: Gérer les coûts de stockage, de traitement et d'analyse par rapport à la valeur obtenue.
- Problématique: Les algorithmes sont durs à exécuter sur des ensembles de données énormes; on travaille souvent sur des sous-ensembles.

## 9. Exemples Concrets et Études de Cas

### Applications Réussies du Data Mining
- Santé: Prédiction d'épidémies, personnalisation des traitements.
- Finance: Détection de fraudes, gestion des risques.
- Commerce de détail: Recommandations de produits, optimisation des prix, gestion des stocks (ex: Walmart).
- Fabrication: Contrôle qualité, optimisation des processus.
- Transport: Prédiction du trafic, optimisation d'itinéraires (ex: Google Maps), maintenance prédictive (ex: GE Aviation).
- Énergie: Prévision de la demande.
- Divertissement: Recommandations de contenu (ex: Netflix, plus de 75% des vues).
- Gouvernement: Amélioration de la sécurité publique, prestations de services (ex: National Weather Service).

### Exemples Concrets de Succès
- Netflix: Algorithmes de recommandation personnalisée.
- American Express: Analyse des transactions pour la détection de fraude.
- Walmart: Optimisation des niveaux de stock.
- GE Aviation: Maintenance prédictive des moteurs d'avion.
- Google Maps: Analyse du trafic en temps réel.
- IBM Watson en Santé: Aide au diagnostic et au traitement du cancer.
- National Weather Service: Amélioration des prévisions météorologiques.
- LinkedIn: Suggestion de connexions professionnelles et d'opportunités d'emploi.

# Cours par Germain Forestier, PhD, Université de Haute-Alsace. QCM de dernière séance: 1 seule réponse, pas de points négatifs.""",
    "kmeans/1.md": """# Notes sur l'Algorithme K-means

L'algorithme K-means est une méthode de clustering (classification non supervisée) dont l'objectif est de construire automatiquement des groupes (ou clusters) compacts au sein des données. C'est un algorithme de partitionnement qui divise le jeu de données en K groupes distincts. Il est relativement simple à comprendre et très efficace.

## 1. Principes Fondamentaux de K-means

- **Classification Non Supervisée**: L'algorithme ne nécessite pas de données labellisées.
- **Partitionnement**: Divise un jeu de données en K sous-ensembles.
- **Paramètre K**: Le "K" dans K-means représente le nombre de clusters que l'on souhaite obtenir. Il doit être fixé a priori, contrairement au clustering hiérarchique.
- **Objectif**: Créer des clusters les plus compacts possible.
- **Moyenne (Means)**: Le terme "means" fait référence au calcul des moyennes pour définir les représentants des clusters.

## 2. Étapes de l'Algorithme K-means

L'algorithme est itératif et vise à améliorer une partition des données.

1. **Sélectionner K**: Choisir le nombre K de clusters à trouver.
2. **Initialisation**: Choisir aléatoirement K points dans l'espace des données pour servir de centres initiaux (ou centroïdes) des clusters. Des travaux existent pour des initialisations plus intelligentes que l'aléatoire simple.
3. **Affectation des objets**: Balayer toutes les données et affecter chaque objet à son cluster dont le centre est le plus proche.
4. **Recalcul des centres**: Pour chaque cluster, recalculer son nouveau centre en calculant la moyenne des objets qui lui ont été affectés. Pour des vecteurs numériques, c'est simple; pour des données plus complexes (ex: images), cela peut être plus délicat.
5. **Répétition**: Répéter les étapes 3 et 4 un certain nombre de fois. L'algorithme s'arrête lorsque:
   - Les résultats sont stables (les centres ne bougent plus, ou très peu d'objets changent de cluster).
   - Un nombre maximal d'itérations est atteint (ex: 10 à 15 itérations).

## 3. Exemple Pratique 1: Données Clients (2D)

- **Contexte**: Analyse de données clients avec "nombre de visites" et "nombre d'achats" (deux dimensions).
- **Processus**:
  1. Initialisation de 3 centres aléatoires.
  2. Affectation des clients (points) aux centres les plus proches.
  3. Recalcul des centres, qui se déplacent.
  4. Nouvelle affectation des clients aux nouveaux centres.
  5. Répétition jusqu'à la stabilisation des centres et des affectations.
- **Interprétation Sémantique**: Après clustering, un expert peut donner du sens aux groupes:
  - Groupe 1: Peu de visites, peu d'achats.
  - Groupe 2: Beaucoup de visites, beaucoup d'achats.
  - Groupe 3 (potentiellement intéressant): Beaucoup de visites, peu d'achats. Ce groupe peut être ciblé pour des promotions afin d'augmenter les achats.

## 4. Exemple Pratique 2: Images Histopathologiques

- **Contexte**: Analyse d'images médicales (biopsies) pour assister au diagnostic.
- **Application**: Clustering des pixels dans l'espace RGB (rouge, vert, bleu) en 3 dimensions, sans considérer la position spatiale des pixels.
- **Objectif**: Regrouper les pixels ayant des couleurs similaires.
- **Résultats**: Affichage de l'image segmentée pour K=2, 3, ou 4 clusters (couleurs).
- **Analyse**: L'expert attribue une sémantique aux clusters de couleurs (ex: cellules violettes sont des lymphocytes, cellules roses des macrophages), permettant de dériver des métriques (nombre de cellules, surface occupée) pour l'aide au diagnostic.
- **Limitation**: Fonctionne bien sur des images très contrastées; moins efficace sur des images avec des nuances de couleurs subtiles.

## 5. Aspects Importants et Considérations

### 5.1 Choix du Paramètre K

- **Délicat**: Impacte directement la qualité des clusters.
- **Méthodes**:
  - Intuition: Basée sur la visualisation ou la connaissance du domaine.
  - Essais multiples: Lancer l'algorithme plusieurs fois avec différentes valeurs de K et étudier les résultats.
  - Critères internes: Méthodes comme le critère du coude (elbow method) ou le score de silhouette.
  - Validation croisée: Tester sur des échantillons de données.
  - Connaissance expert: Demander à un spécialiste du domaine (ex: médecin).

### 5.2 Choix de la Distance (Métrique)

- **Essentiel**: Détermine comment la similarité entre objets est calculée.
- **Classique**: La distance euclidienne est la plus utilisée pour les vecteurs numériques.
- **Autres**: Distance de Manhattan, distance Cosine, etc. Le choix dépend de la nature des données et des caractéristiques à prendre en compte.
- **Exploration**: Il est possible de réaliser plusieurs clusterings avec différentes métriques pour observer les variations.

### 5.3 Critères d'Arrêt des Itérations

- **Stabilité des centres**: L'algorithme s'arrête lorsque les centres des clusters ne bougent plus significativement.
- **Stabilité des affectations**: Plus aucun ou très peu d'objets ne changent de cluster.
- **Erreur résiduelle**: Observer la compacité des clusters et arrêter quand elle n'évolue plus beaucoup.
- **Nombre d'itérations fixe**: Définir un nombre maximum d'itérations (ex: 10, 15).

### 5.4 Sensibilité aux Conditions Initiales

- **Optimisation Locale**: K-means est un algorithme d'optimisation locale.
- **Variabilité des Résultats**: Une initialisation aléatoire des centres peut conduire à des résultats différents à chaque exécution.
- **Solutions**:
  - Lancer K-means plusieurs fois et choisir le meilleur résultat.
  - Utiliser des variantes d'initialisation comme K-means++.

### 5.5 Sensibilité aux Valeurs Aberrantes

- **Impact**: Les valeurs aberrantes peuvent fortement influencer le calcul des moyennes et déformer les clusters.
- **Solutions**:
  - Pré-traitement des données (data cleaning).
  - Mise à l'échelle des données.
  - Utiliser des variantes de K-means moins sensibles, comme K-médoïdes.

## 6. Variantes de K-means

Il existe des centaines de variantes de K-means pour adresser ses limitations ou adapter à des besoins spécifiques.

- **K-médoïdes (K-Medoids)**:
  - Au lieu de calculer une moyenne abstraite comme centre, il choisit un objet existant des données du cluster comme représentant (médoïde).
  - Moins sensible aux valeurs aberrantes car le médoïde est une donnée réelle.
- **Fuzzy K-means (C-Means Flou)**:
  - Permet à un objet d'appartenir à plusieurs clusters avec un certain degré d'appartenance (partitionnement "flou" ou "doux").
  - Contrairement à K-means classique qui fait un partitionnement "dur" (un objet = un cluster).
- **K-means++**:
  - Améliore la phase d'initialisation des centres.
  - Sélectionne les centres initiaux de manière plus intelligente pour mieux couvrir l'espace de données, ce qui améliore la stabilité et la qualité des clusters finaux.
- **Bisecting K-means**:
  - Découpe itérativement les clusters trop grands en deux jusqu'à atteindre le nombre K désiré.

## 7. Avantages et Inconvénients de K-means

### 7.1 Avantages

- **Efficacité**: Algorithme rapide, surtout sur de grands jeux de données.
- **Interprétabilité**: Le centre de chaque cluster (la moyenne) fournit un prototype facile à interpréter par un expert.
- **Simplicité**: Peu de paramètres à définir (K, métrique, nombre d'itérations).

### 7.2 Inconvénients

- **Choix de K**: La détermination du nombre optimal de clusters (K) peut être difficile.
- **Contrainte de Prototype**: Avec la distance euclidienne, K-means tend à former des clusters de forme sphérique. Il peut mal performer sur des clusters de formes arbitraires.
- **Sensibilité à l'Initialisation**: En raison de son aspect d'optimisation locale, des initialisations défavorables peuvent mener à des résultats sous-optimaux. Il est recommandé de lancer l'algorithme plusieurs fois ou d'utiliser K-means++ pour plus de robustesse.
- **Sensibilité aux Aberrations**: Les points extrêmes peuvent fausser les centres des clusters.

## 8. Conclusion

K-means est un algorithme de clustering très connu, puissant et largement utilisé. Malgré ses inconvénients, il reste une base solide pour l'analyse de données non supervisée, en particulier grâce à sa simplicité et son efficacité. De nombreuses variantes existent pour pallier ses limitations et l'adapter à des contextes spécifiques.""",
    "kmeans/2.md": """# K-Means Clustering: Comprehensive Notes

## Main Takeaway
K-Means clustering is a widely used unsupervised learning technique that partitions data into k distinct, non-overlapping groups (clusters) based on similarity. Its primary objective is to minimize intra-cluster variance, thereby enhancing homogeneity within clusters. The algorithm operates iteratively, assigning data points to the nearest cluster centroid and then recomputing these centroids, continuing until a stopping criterion is met. While valued for its simplicity and efficiency, especially with large datasets, K-Means requires the number of clusters (k) to be predefined and is notably sensitive to the initial selection of centroids and the presence of outliers.

## 1. Introduction to K-Means Clustering
- **Source:** Germain Forestier, PhD, Université de Haute-Alsace.
- **Clustering:**
  - A technique in unsupervised learning.
  - Groups data based on similarity.
- **Partitioning Clustering:**
  - Divides the dataset into distinct, non-overlapping groups.
- **K-Means Clustering:**
  - A widely-used method where k specifies the number of clusters.
  - Objective: Minimize intra-cluster variance to enhance homogeneity.
  - Significance: Known for its simplicity and efficiency, especially in large datasets.

## 2. Understanding the K-Means Algorithm

### Overview
- Iteratively constructs and refines partitions to form satisfactory clusters.
- Involves calculating the mean to determine cluster centers (centroids).

### Steps of the K-Means Algorithm
1. Select k clusters to form.
2. Initialize by choosing k random points as cluster centers.
3. Assign each point to the nearest cluster center.
4. Recompute the centroids for each cluster by calculating the mean of all points assigned to that cluster.
5. Repeat the assignment and update steps until the stopping criterion is met (e.g., no points change clusters, a fixed number of iterations like 10-15 iterations).

### Exercise Example: E-commerce Customer Study
- **Objective:** Apply K-Means to study customers based on 'Number of Visits' and 'Number of Purchases'.
- **Data:** 10 clients with 2 characteristics (x, y) representing visits and purchases.
- **Initial Centers for 3 Clusters:**
  - Cluster 1: (1.5, 3.0)
  - Cluster 2: (4.0, 0.5)
  - Cluster 3: (2.5, 5.0)
- **Example Cluster Means (after initial iteration):**
  - Cluster 1: (1.50, 1.50)
  - Cluster 2: (4.66, 2.33)
  - Cluster 3: (5.33, 5.66)

## 3. Application Examples
- **Histopathological Image Processing:**
  - Goal: Automatically identify different structures in images captured with an electron microscope.
  - The colors in the image correspond to various structures.
  - Clustering results shown for 2, 3, and 4 clusters.

## 4. Fundamentals of K-Means

### Choosing 'k' in K-Means
- **Importance:** Directly impacts clustering quality; balances sensitivity to noise/outliers.
- **Methods for Determining 'k':**
  - Elbow Method: Identify the 'elbow' point in the plot of squared distances.
  - Silhouette Score: Higher scores indicate better-defined clusters.
  - Cross-validation: Assess cluster stability and predictive strength.
  - Practical Tips: Use domain knowledge; test various k values and evaluate.

### Distance Metrics in K-Means
- **Role:** Define similarity between objects; influence cluster shape and size.
- **Common Distance Metrics:**
  - Euclidean Distance: Straight line distance in space.
  - Manhattan Distance: Sum of absolute differences.
  - Cosine Similarity: Cosine of the angle between vectors.
- **Choosing the Right Metric:** Select based on data nature and clustering goals; experiment.

### Convergence of K-Means
- **Definition:** Occurs when cluster centroids stabilize between iterations.
- **Indicators of Convergence:**
  - Stable Centroids: No significant change in centroid positions (e.g., "stabilité 3 centres").
  - Assignment Stability: Points consistently belong to the same clusters.
  - Minimal Error Reduction: Small changes in the objective function.
- **Ensuring Convergence:** Use k-means++ for effective initial centroid selection; set a convergence threshold for centroid adjustments.

## 5. Practical Challenges and Solutions

### Common Issues with K-Means
- **Sensitivity to Initial Conditions:**
  - Clusters heavily depend on initial centroid selection, leading to variability.
  - Mitigation: Use k-means++ for better initial centroids.
- **Sensitivity to Outliers:**
  - Outliers can distort centroid calculation, resulting in biased clusters.
  - Mitigation: Use robust methods or consider algorithms like k-medoids.

### Improving K-Means Performance
- **Methods for Choosing Initial Centroids:**
  - k-means++: Optimizes initial centroid selection, leading to more consistent and efficient clustering, and reduces iterations.
- **Techniques for Outlier Handling:**
  - Data Cleaning: Remove noise and outliers before clustering.
  - Robust Scaling: Use scaling techniques less sensitive to outliers.

### Variants of K-Means
- **k-medoids:** Chooses actual data points (medoids) as centers, offering more robustness to outliers.
- **Fuzzy k-Means:** Each point belongs to all clusters with different degrees of membership, suitable for overlapping clusters.
- **k-means++:** (As mentioned above) Enhances the initialization phase of K-Means to improve cluster quality and convergence speed.
- **Bisecting k-Means:** A divisive clustering algorithm that iteratively splits clusters to achieve the desired number.

## 6. Advantages and Disadvantages of K-Means

### Advantages
- **Efficiency:** Operates in linear time, making it very fast.
- **Interpretability:** Clusters are easy to understand, centered around clear prototypes.
- **Simplicity:** Minimal parameters required (just the number of clusters and iterations).

### Disadvantages
- **Fixed k:** Number of clusters must be specified in advance.
- **Prototypical Constraints:** Limited to finding spherical-shaped clusters around centroids.
- **Initialization Sensitivity:** Outcome heavily depends on the initial position of centroids.""",
    "knn/1.md": """# Le Plus Proche Voisin (k-NN)

## Main Takeaway
Le k-Plus Proche Voisin (k-NN) est un algorithme d'apprentissage automatique basé sur des instances (lazy learning) qui permet de faire de la classification et de la régression en se basant sur la classe majoritaire ou la moyenne des k objets les plus proches dans l'ensemble d'entraînement. Simple à comprendre et à mettre en œuvre, il est adaptable mais présente des défis majeurs en termes de coût computationnel pour de grands volumes de données et nécessite une préparation rigoureuse des données, notamment la normalisation des caractéristiques et le choix optimal du paramètre k.

## Introduction
- Algorithme basé sur des instances (instance-based learning ou lazy learning).
- Permet de faire de la classification (prédire valeur discrète) et de la régression (prédire valeur continue). L'accent est mis sur la classification.
- Fonctionnement : Classifie un nouvel objet en se basant sur la classe majoritaire au sein des k plus proches voisins.
- k est le nombre de voisins à considérer et est un paramètre de l'algorithme.
- 1-Plus Proche Voisin (1-NN) : Cas particulier où on cherche uniquement l'objet le plus proche.
- Le k-NN est une famille d'algorithmes avec de nombreuses versions selon le choix de la distance, de k, de la pondération, etc.

## Concepts Clés

### Mesure de Distance
- Nécessaire pour définir la notion de "proximité".
- Distance Euclidienne : Très classique et largement utilisée pour les vecteurs de valeurs numériques.
- Autres distances : Manhattan, Minkowski, similarité cosinus. Le choix dépend du type de données et des caractéristiques souhaitées.

### Apprentissage Paresseux (Lazy Learning)
- Dit "paresseux" car il n'y a pas d'étape de construction de modèle à l'avance.
- Le "modèle" est constitué par l'intégralité des données d'entraînement.
- Lors de l'inférence (classification d'un nouvel objet), il est comparé à tous les objets du jeu d'entraînement.

## Historique et Applications
- Développé dans les années 1950.
- Formalisé par Cover et Hart en 1967.
- Applications courantes :
  - Vision par ordinateur / Reconnaissance d'images.
  - Systèmes de recommandation.
  - Médecine / Diagnostic médical (facilement explicable).
  - Recherche d'information.
- Implémenté dans l'intégralité des librairies d'apprentissage automatique.

## Avantages
- Simplicité : Facile à comprendre et à expliquer (ex: "chercher le plus proche").
- Polyvalence : Utilisable pour la classification et la régression.
- Rapidité de mise en place : Pas de phase de "training" coûteuse en temps de construction de modèle.
- Non-paramétrique : Ne fait pas d'hypothèses sur la distribution des données, peut construire des frontières de classe complexes.
- Mise à jour facile : Les nouvelles données sont simplement ajoutées au jeu d'entraînement et prises en compte instantanément pour les classifications futures, sans reconstruire de modèle.
- Explicabilité : Facile d'expliquer une prédiction en montrant les voisins les plus proches (particulièrement utile en diagnostic médical).

## Inconvénients et Défis

### Coût Algorithmique Élevé (Temps et Mémoire)
- Problématique pour les gros volumes de données ou un grand nombre de caractéristiques.
- Chaque classification implique la recherche des k plus proches voisins dans tout l'ensemble d'entraînement.
- Le calcul de distance peut être coûteux pour des données complexes (images, texte).

### Sensibilité au Bruit
- Un point bruité ou outlier peut fortement influencer la classification des objets à proximité.
- Une petite valeur de k augmente cette sensibilité.

### Choix de k Difficile
- La valeur de k a un impact significatif sur les performances.
- Petit k : Sensible au bruit, risque de surapprentissage (overfitting), frontières de classification complexes.
- Grand k : Plus résistant au bruit, risque de sous-apprentissage (underfitting), frontières de décision plus lisses.
- Méthodes pour choisir k : Essai-erreur, validation croisée (analyse du taux d'erreur).
- Pour les données binaires, choisir un k impair (ex: 3 ou 5) aide à éviter les égalités de vote.

### Nécessité de Normalisation des Caractéristiques
- Indispensable pour que toutes les caractéristiques aient le même poids dans le calcul de distance.

## Exemple Visuel (Données IRIS)
- Utilisation des données IRIS : 150 objets (50 Setosa, 50 Vericolor, 50 Virginica).
- Un jeu d'entraînement et un jeu de test sont définis.
- Pour chaque objet du jeu de test, on cherche le plus proche voisin (ici, en 1-NN) dans le jeu d'entraînement pour assigner sa classe.

## Implémentation Naïve et Optimisations
- Fonction de classification naïve (1-NN) :

```
Pour chaque objet_test à classer:
    Trouver dans l'ensemble d'entraînement (train) l'objet_train le plus proche.
    Assigner la classe de l'objet_train trouvé à l'objet_test.
```

- Optimisations : Techniques pour accélérer le processus de recherche des voisins (ex: calcul de lower bound, pruning, mise en cache de mesures de distance).

## Pondération des Voisins
- Version de base : Vote à la majorité simple (tous les voisins ont le même poids).
- Alternative (pondération par distance) :
  - Attribuer un poids proportionnel à la distance de chaque voisin. Les voisins plus proches ont un poids plus important.
  - Utile pour résoudre les égalités de vote et donner plus de pertinence aux objets très proches.
  - Exemple de formule : \\( \\frac{1}{\\text{distance}^2} \\) (ou d'autres formules).

## Normalisation des Données (Mise à l'Échelle)
- Principe fondamental : Systématiquement mettre à l'échelle ou normaliser les données avant tout calcul de distance.
- Objectif : Ramener les caractéristiques à une même échelle (ex: entre 0 et 1).
- Exemple : La normalisation Min-Max est courante :

\\[
\\text{Valeur_normalisée} = \\frac{\\text{Valeur_originale} - \\text{Min_caractéristique}}{\\text{Max_caractéristique} - \\text{Min_caractéristique}}
\\]

- Raison : Sans normalisation, les caractéristiques avec de plus grandes échelles (ex: prix vs. taille en mètres carrés) domineraient le calcul de distance, indépendamment de leur importance réelle.

## Données Catégorielles
- Problème : Les mesures de distance standards sont conçues pour des valeurs numériques.
- Solution : One-Hot Encoding (encodage binaire).
- Convertit une caractéristique catégorielle en un vecteur numérique binaire.
- Ex: Une caractéristique "Couleur" avec les valeurs "red", "green", "blue" serait encodée en :
  - "red" -> [1, 0, 0]
  - "green" -> [0, 1, 0]
  - "blue" -> [0, 0, 1]
- Inconvénient : Peut augmenter considérablement le nombre de colonnes si une caractéristique prend beaucoup de valeurs possibles, ce qui impacte la complexité.

## Résumé et Points Clés
- Algorithme adaptable pour classification, régression, recommandation.
- Puissant et simple à expliquer, mais demande un travail significatif sur les données.
- Le choix de la mesure de distance est crucial et dépend du type de données.
- La complexité temporelle est le défi majeur avec de grands datasets.
- Le k-NN est une famille d'algorithmes avec de nombreuses variantes.
- Importance du prétraitement des caractéristiques : mise à l'échelle (normalisation) et gestion des données manquantes.
- Le paramètre k est central et doit être réglé avec soin.
- Avantages notoires : Explicabilité (ex: diagnostic médical) et facilité de mise à jour des données sans reconstruction du modèle.""",
    "knn/2.md": """# k-Nearest Neighbors (k-NN)

k-Nearest Neighbors (k-NN) is an instance-based learning algorithm applicable for both classification and regression tasks. It operates as a lazy learning method, meaning no explicit model is built during training; computations occur at the time of prediction. Its effectiveness relies on calculating distances between data points, making preprocessing steps like feature scaling and appropriate handling of categorical features crucial. Challenges include computational cost for large datasets and sensitivity to the choice of 'K' and noisy data.

## Definition and Overview

- **Definition**: An instance-based learning algorithm for classification and regression.
- **How it Works**: Classifies new data points based on the majority class (or average for regression) of its K nearest neighbors in the training data.
- **Distance Metrics**: Uses metrics like Euclidean, Manhattan, and Minkowski to measure closeness.
- **Lazy Learning**: No model is built during training; all computations occur during prediction.
- **Use Cases**: Applied in image recognition, recommender systems, and pattern recognition.

## Historical Background

- **Origins**: Developed in the 1950s, rooted in pattern recognition theory.
- **Formalized**: By Thomas Cover and Peter Hart in 1967.
- **Development**: Enhanced with weighted voting and various distance metrics.
- **Modern Usage**: Applied in computer vision, recommender systems, healthcare, and available in many ML libraries.
- **Challenges and Evolution**: Faces computational complexity with large datasets; ongoing research focuses on optimization and parallelization.

## Advantages and Disadvantages

### Advantages
- **Simplicity**: Easy to understand and implement.
- **Versatility**: Can be used for both classification and regression.
- **No Model Training**: Lazy learning approach.
- **Non-Parametric**: Makes no assumptions about underlying data distribution.

### Disadvantages
- **Computational Cost**: Expensive in terms of time and memory, especially with large datasets.
- **Sensitive to Noisy Data**: Outliers can negatively impact performance.
- **Choice of 'K'**: Selecting an optimal value for 'K' can be challenging.
- **Scaling Required**: Requires feature scaling to prevent distance bias.

## Understanding the k-NN Algorithm

### Pseudo Code: 1-Nearest Neighbor (1-NN)

This is a simplified version for k=1.

```python
function 1NN(X_train, Y_train, x):
    closestDistance = infinity
    closestClass = null
    # Compute distances between x and all samples in X_train
    for i = 1 to length(X_train):
        dist = distance(x, X_train[i])
        if dist < closestDistance:
            closestDistance = dist
            closestClass = Y_train[i]
    return closestClass
```

- **Input**: Training data (X_train, Y_train), test sample x.
- **Output**: Predicted class for the test sample x.
- **Distance**: A distance function (e.g., Euclidean, Manhattan).

## Distance Metrics

- **Euclidean Distance**: Commonly used, computed as the square root of the sum of squared differences.  
  $d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$
- **Manhattan Distance**: Sum of the absolute differences between corresponding elements.  
  $d(x, y) = \\sum_{i=1}^{n} |x_i - y_i|$
- **Minkowski Distance**: Generalization of Euclidean (p=2) and Manhattan (p=1) distances.  
  $d(x, y) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{1/p}$
- **Cosine Similarity**: Measures the cosine of the angle between two vectors, often used with text data.  
  $\\text{similarity}(x, y) = \\frac{x \\cdot y}{\\|x\\| \\cdot \\|y\\|}$

## Choosing the Value of 'k'

The parameter K represents the "number of neighbors to inspect".

### Small Values of 'k'
- Sensitive to noise.
- Can lead to overfitting.
- Often results in a complex decision boundary.

### Large Values of 'k'
- More resistant to noise.
- Can lead to underfitting.
- Tends to produce a smoother decision boundary.

### Common Strategies
- **Cross-validation**: To test different 'k' values on the training set (do not test on the final test set).
- **Choosing an odd value for binary classification** to avoid ties.
- **Analyzing the error rate** for various 'k' values.

## Weighted vs. Unweighted Voting

### Unweighted Voting
- Every neighbor has an equal vote.
- Simple majority rule determines the class.

### Weighted Voting
- Neighbors' votes are weighted by their distance.
- Closer neighbors have a greater influence.
- **Example**: $w_i = \\frac{1}{d(x, x_i)^2}$ (weight is inverse square of distance).
- **Comparison**: Weighted voting offers more nuanced predictions than unweighted voting.

## Preprocessing for k-NN

### Feature Scaling and Normalization
- **Feature Scaling**: Brings all features to the same scale, preventing features with larger scales from dominating distance computations.
- **Normalization (Min-Max Scaler)**: A special case of scaling that transforms the range of features to [0, 1].  
  $x_{\\text{norm}} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}$
- **Importance in KNN**: Ensures all features contribute equally to distance computation and prevents undue influence of features with larger numeric ranges.
- **Outliers**: Can significantly affect min-max scaling, potentially distorting the [0,1] range for the majority of data.

### Handling Categorical Features
- **Issue**: k-NN relies on distance computations, which are not directly applicable to categorical data.
- **Methods**:
  - **One-Hot Encoding**: Converts categories into binary (0 or 1) columns. This is suitable for nominal (unordered) categorical features.
  - **Ordinal Encoding**: Assigns ordered numbers to categories that inherently have an order (e.g., "un peu, beaucoup, à la folie").
- **Distance Metrics**: Hamming Distance is suitable for binary representations (e.g., after one-hot encoding).
- **Challenges**:
  - One-Hot Encoding can increase dimensionality ("sparse matrix").
  - Ensuring meaningful distances between categories.

## Conclusion
- **Versatile Algorithm**: Suitable for classification, regression, and recommendation systems.
- **Simple Yet Powerful**: Effective with appropriate feature engineering.
- **Distance Metrics**: Choice of metric is crucial for performance.
- **Challenges**: Computationally intensive (especially with large datasets), may struggle in high-dimensional spaces. However, it is adaptable and can be updated with new data easily, making it suitable for "big data" if optimized.
- **Advanced Techniques**: Includes Kernelized k-NN, ensembles (e.g., combining k=1, k=2, k=3, similar to Mixture of Experts in LLMs), etc.
- **Practical Considerations**: Feature scaling, handling missing data, and selecting an optimal 'k' are vital.
- **Real-world Applications**: Used in medical diagnosis, financial forecasting, and more.""",
    "neural-network/1.md": """# Notes de Cours : Introduction aux Réseaux de Neurones

Ce cours de fouille de données est dédié aux réseaux de neurones, couvrant leur historique, les principes fondamentaux du Perceptron, l'algorithme d'apprentissage, un exercice pratique et une introduction au Deep Learning.

## 1. Historique des Réseaux de Neurones

- **Années 1960**
  - Rosenblatt propose le Perceptron, un classifieur binaire.
  - Minsky et Papert mettent en évidence l'incapacité des Perceptrons simples à résoudre des problèmes non linéaires (ex: problème XOR).
  - "AI Winter": Période de perte de confiance due à cette limitation.

- **1974**
  - Werbos propose l'algorithme de Rétropropagation du gradient (Backpropagation), permettant de résoudre les problèmes non linéaires en utilisant des réseaux multicouches.
  - L'algorithme est réutilisé et popularisé par Mel, Hinton et Williams pour l'entraînement des réseaux multicouches.

- **Années 1990**
  - Apparition des CNN (Convolutional Neural Networks) pour la classification d'images, proposés par Yann LeCun.
  - Yann LeCun reçoit le Prix Turing (équivalent Nobel en informatique) pour ses travaux sur les CNN.

## 2. Le Perceptron : Bases et Fonctionnement

- **Définition**: Un neurone est une unité de calcul qui prend des entrées, effectue un calcul et renvoie une sortie.
- **Entrées**: n entrées (x1 à xn) et un biais x0 toujours égal à 1.
- **Paramètres**: Les poids (weights) W0 à WN sont les valeurs apprises pendant l'entraînement.
- **Calcul de la sortie (version simple)**:
  1. Calcul d'une somme pondérée des entrées et des poids (∑(xi * Wi)).
  2. Cette somme est passée à une fonction d'activation qui calcule la sortie finale du neurone.
- **Limitation**: Le perceptron de base modélise des décisions linéaires et ne peut pas résoudre des problèmes non linéaires.
- **Représentation schématique**: Entrées (x0...xn) -> Poids (W0...Wn) -> Somme pondérée -> Fonction d'activation -> Sortie.

## 3. Algorithme d'Apprentissage du Perceptron : Correction par Erreur

- **Principe**: Ajuster les poids du réseau pour maximiser le taux de bonnes réponses.
- **Étapes**:
  1. Initialiser le perceptron et ses poids à des valeurs arbitraires.
  2. Pour chaque exemple d'entraînement:
     - Présenter l'exemple au réseau.
     - Calculer la sortie.
     - Comparer la sortie avec la classe attendue.
     - Si la classification est incorrecte, ajuster les poids.
  3. L'algorithme s'arrête lorsque tous les exemples sont correctement classés et qu'aucun changement de poids n'est nécessaire (stabilité).
- **Formule de mise à jour des poids**:
  $W_{\\text{nouveau}} = W_{\\text{précédent}} + \\eta \\times (C - O) \\times X_{\\text{entrée}}$
- **η**: Taux d'apprentissage (non explicitement mentionné mais implicite dans le coefficient multiplicateur, souvent appelé alpha).
- **C**: Classe attendue (cible).
- **O**: Sortie calculée par le réseau.
- **X_entrée**: Valeur de l'entrée correspondante.
- **Si C = O, (C - O) est 0, donc les poids ne sont pas modifiés.**
- **Note**: Modifier les poids peut altérer la classification correcte d'exemples précédemment bien classés, nécessitant plusieurs passes sur l'ensemble des exemples.

## 4. Exercice Pratique : Apprentissage du "OU Booléen"

- **Objectif**: Entraîner un perceptron à apprendre la fonction logique OU.
- **Entrées**: x0=1 (biais), x1 et x2 (binaires: 0 ou 1).
- **Sortie attendue**: x1 OU x2.
- **Processus**:
  1. Initialiser les poids arbitrairement (ex: 0, -1, 1 pour W0, W1, W2).
  2. Parcourir séquentiellement les exemples du tableau d'entraînement.
  3. Pour chaque exemple, calculer la sortie et mettre à jour les poids si nécessaire.
  4. Repasser sur l'ensemble des exemples si des poids ont été modifiés, jusqu'à ce que les poids soient stables et que tous les exemples soient bien classés.
- **Résultat stable (ex)**: Poids finaux de 0, 1, 1 pour W0, W1, W2 (pour le ou booléen).

## 5. Deep Learning (Apprentissage Profond)

- **Définition**: Terme "à la mode" désignant des réseaux de neurones profonds, c'est-à-dire des réseaux avec plusieurs couches de neurones.
- **Caractéristiques**: Contiennent de nombreuses couches et un très grand nombre de paramètres (parfois des milliards dans les réseaux modernes).
- **Principe fondamental**: Le même que celui du perceptron simple (unités de calcul qui prennent, calculent et transmettent de l'information), mais avec une complexité accrue due à la profondeur.
- **Succès**: Capacité à entraîner des réseaux multicouches pour apprendre des décisions non linéaires complexes.
- **Exemples d'architectures notables**:
  - **LeNet-5 (1998)**:
    - Conçu pour la reconnaissance de chiffres manuscrits (dataset MNIST).
    - Contenait environ 60 000 paramètres.
    - Structure typique: couches de convolution, couches de sous-échantillonnage, couche entièrement connectée, distribution de probabilité en sortie.
  - **AlexNet (2012)**:
    - A révolutionné la classification d'images sur le benchmark ImageNet.
    - Contenait environ 60 millions de paramètres.
  - **Inception (2014)**:
    - Proposé pour ImageNet.
    - Particularité: Utilise des convolutions de tailles différentes au même niveau de couche pour capturer l'information à diverses échelles.
  - **VGG16 (2015)**:
    - Contenait environ 138 millions de paramètres.
- **Évolution et facteurs clés**:
  - Augmentation constante de la profondeur des réseaux et du nombre de paramètres.
  - Performance accrue du matériel (GPU): plus de puissance de calcul et de mémoire, permettant d'entraîner des modèles de plus en plus complexes et profonds.
- **Architectures plus récentes**:
  - **Transformers (2017)**: "Attention Is All You Need" (papier de recherche). Une architecture particulière qui a marqué une évolution importante.
- **Le principe d'entraînement (ajustement des poids pour corriger les erreurs) reste le même, même si les architectures internes et les "briques" des réseaux deviennent plus complexes.**

## 6. Conclusion Générale

- Le concept d'entraînement des perceptrons, c'est-à-dire ajuster les poids pour corriger les erreurs, demeure la base fondamentale de tous les réseaux de neurones, y compris les plus profonds et complexes actuels.
- L'apprentissage profond applique ce principe pour entraîner des réseaux multicouches capables de résoudre des problèmes complexes et non linéaires.
- Ces techniques sont utilisées dans de nombreuses applications modernes (ex: architectures type Transformers).""",
    "neural-network/2.md": """# Notes on Data Mining - Neural Networks

## Main Takeaway

This document provides an overview of Neural Networks and Deep Learning, covering their history, the fundamental concept of the perceptron and its learning algorithm, and the evolution of deep neural network architectures. The core idea of adjusting weights to correct errors in perceptrons forms the foundation for modern, multi-layer neural networks, which are crucial for solving complex, non-linear problems across various applications.

## 1. Neural Networks

**Source:** Germain Forestier, PhD, Université de Haute-Alsace (https://germain-forestier.info)

### History

- 1960s: Rosenblatt programmed a perceptron (binary classifier).
- 1969: Minsky and Papert highlighted the problem of nonlinear decision boundaries.
- 1970s: AI Winter.
- 1974: Werbos proposed the backpropagation algorithm in his thesis.
- 1986: Rumelhart, Hinton, and Williams rediscovered the backpropagation algorithm.
- 1990s: Convolutional neural networks by Yann LeCun.

**Source:** https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html

### Perceptron

#### Key Concepts

- Takes n input values (x1, ..., xn), with x0 = 1 (bias).
- Computes an output o.
- Parameters to learn are weights (w0, ..., wn).
- Output o is computed as Σ wi xi.
- This sum is passed through an activation function.
- Activation function: Heaviside function:

  ```
  f(x) = 1 if x > 0
         0 otherwise
  ```

- Perceptrons model linear decision boundaries but cannot solve non-linear problems.

#### Structure Diagram

```
x0 ---- w0 ----
x1 ---- w1 ----
x2 ---- w2 ----
...             
xn ---- wn ---- > Σ > Activation function > Output (o)
```

- Entry weights are learned parameters.

### Perceptron Learning Algorithm

#### "Classic" Algorithm

1. Initialize perceptron weights to arbitrary values.
2. Each time a new example is presented, adjust weights based on correct/incorrect classification.
3. Algorithm stops when all examples are presented without any weight adjustments.

#### Error Correction Algorithm

- **Input:** Set of examples (x1, ..., xn) and expected outputs (c).
- Random initialization of weights wi (for i from 0 to n).
- Repeat:
  - Take an example.
  - Compute perceptron output o for the example.
  - Weight update:
    - For i from 0 to n:
      - wi = wi + (c - o)xi
    - Endfor
  - Endrepeat
- **Output:** Perceptron defined by (w0, w1, ..., wn).
- **Risk:** Oscillation; requires a defined number of epochs.

#### Perceptron Learning Example: Boolean OR

- Initialization: w0 = 0, w1 = 1, w2 = -1.
- Examples Order: (1,0,0,0), (1,0,1,1), (1,1,0,1), (1,1,1,1) (where x0, x1, x2, c).
- The document provides a detailed step-by-step unrolling of the learning process, showing weight adjustments over 10 iterations. The weights adjust based on the error (c - o) until the perceptron converges to correctly classify all OR inputs.

## 2. Deep Learning

### Definition

Deep Learning ≡ Deep Neural Networks ≡ Multi-layer Networks.

### Success

Lies in training multi-layer neural networks to learn nonlinear decisions.

"Deep Learning: a fancy word for deep neural networks."

### Key Deep Neural Network Architectures

#### LeNet-5 (1998): For MNIST-like tasks (document recognition).

**Features:**

- Designed for grayscale images.
- Convolutions followed by pooling, then a final MLP.
- Uses linear functions: sigmoid/tanh.
- Activation functions applied after pooling.
- Same filters applied to each channel.
- Contains 60,000 learnable parameters.

#### AlexNet (2012): For ImageNet.

**Features:**

- Designed for RGB images.
- Similar to LeNet-5 but significantly larger.
- Contains 60 million parameters.
- Uses the ReLU (Rectified Linear Unit) activation function.
- Had the most significant impact on deep learning.

#### Inception (2014): For ImageNet.

**Features:**

- Implements 1x1 convolution to reduce computational cost.
- Uses multiple filter sizes in parallel.
- Eliminates the need to manually choose these hyperparameters.

#### VGG-16 (2015): For ImageNet.

**Features:**

- Same filter size (3x3) for all layers.
- Same max-pooling size (2x2) for all layers.
- Contains 138 million learnable parameters.
- No need to manually choose filters or pooling sizes.

## 3. Conclusion

### Key Points

- The concept of training perceptrons (adjusting weights to correct errors) is the fundamental principle of modern neural networks.
- Deep learning extends this by using multi-layer networks to solve more complex problems.
- The same training approach applies across various applications (e.g., image processing, speech recognition, language models like GPT).""",
    "time-series/1.md": """# Time Series Data Mining: Comprehensive Notes

This document provides a comprehensive overview of time series data mining, covering definitions, characteristics, types, decomposition, statistical feature extraction, and various analytical techniques including forecasting, imputation, classification, extrinsic regression, handling unequal lengths, and dimensionality reduction. It emphasizes the crucial role of temporal order in time series analysis.

## 1. Introduction to Time Series

### What are Time Series?

- **Definition**: Ordered sequence of data points, typically measured at successive points in time.
- **Key Property**: Temporal order matters, unlike in tabular datasets.
- **Examples**:
  - Air Passengers Time Series (1950-1960 data shown)
  - Natural Sciences: Climate measurements, seismic activity
  - Engineering: Sensor readings, energy consumption data
  - Finance: Stock prices, exchange rates
  - Healthcare: Patient monitoring, ECG signals

### Univariate and Multivariate Time Series

- **Univariate Time Series**:
  - Observations of a single variable over time.
  - Example: Daily temperature readings, stock prices.
- **Multivariate Time Series**:
  - Observations of multiple variables over time.
  - Captures relationships or dependencies between variables.
  - Example: Weather data (temperature, humidity, wind speed), financial market (Stock 1, Stock 2, Stock 3 from 2023-01-01 to 2023-04-01).

### Comparison to Tabular Data: Importance of Ordering

- **Tabular Data**: No inherent order between rows.
- **Time Series Data**:
  - Ordering of observations is crucial; it reflects temporal relationships.
  - Patterns like trends, seasonality, and cyclic behaviors rely on temporal order.
  - Reversing time order disrupts interpretability and prediction accuracy.

### Decomposition of Time Series

- **Time Series Components**:
  - **Trend**: Long-term progression or direction in the data.
  - **Seasonality**: Regular and repeating patterns over fixed time periods.
  - **Cyclic Patterns**: Irregular, long-term fluctuations not tied to fixed intervals.
  - **Noise**: Random variations or irregularities with no discernible pattern.
- **Example**: Sales data may exhibit an upward trend, annual seasonality, economic cycles, and random noise.

## 2. Statistics-Based Feature Extraction

### Mean, Standard Deviation, and Z-Normalization

- **Mean (µ)**: Average value of a time series.
  - Formula: µ = (1/n) * Σ(xi)
- **Standard Deviation (σ)**: Measure of the spread or dispersion of the data.
  - Formula: σ = sqrt((1/n) * Σ(xi - µ)^2)
- **Z-Normalization**: Standardizes time series to have mean 0 and standard deviation 1.
  - Formula: zi = (xi - µ) / σ
  - Removes scale and offset differences for better comparison.

### Temporal Auto-Correlation

- **Definition**: Measures the correlation of a time series with a lagged version of itself.
- Indicates how past values influence future values within the series.
- Formula: ρk = Σ(xt - µ)(xt+k - µ) / Σ(xt - µ)^2
- k: Lag; µ: Mean of the series.
- Applications: Identifying repeating patterns or dependencies; used in models like ARIMA and forecasting tasks.

### Fourier Transform for Time Series

- **Definition**: Decomposes a time series into its frequency components.
- Represents the series as a sum of sinusoidal functions (sines and cosines).
- Key Formula: F(f) = Σ(xt * e^(-2πi * ft/N))
- F(f): Complex frequency component at index f; xt: Signal value at time t.
- Applications: Identifying dominant frequencies in the data; filtering noise or periodicity detection.
- Note (from handwritten source): Fourier Transform might be considered more reliable for outliers, as a perturbed mean might cause a "shift to the right" (dicaler a' choite).

### catch22: Canonical Time-Series Characteristics

- **Definition**: CAnonical Time-series CHaracteristics with 22 features.
- A set of handcrafted statistical features designed for time series classification and analysis.
- Key Features: Includes measures of distribution, autocorrelation, entropy, and non-linear properties (e.g., Mean autocorrelation, fluctuation analysis, entropy metrics).
- Advantages: Computationally efficient and interpretable; provides a compact feature set for quick analysis and classification tasks.

## 3. Time Series Analysis

### General Time Series Analysis Tasks

- Classification
- Clustering
- Extrinsic Regression
- Prototyping / Generation
- Forecasting
- Anomaly Detection

### 3.1 Time Series Forecasting/Regression

- **Definition**: Predicting future values based on past data. A regression problem where temporal ordering plays a key role.
- Key Idea: Given a time series X = {x1, x2, ..., xt}, predict xt+1, xt+2, ....
- Applications: Financial market prediction, demand forecasting, climate modeling and weather prediction.
- Note (from handwritten source): Regression is mentioned possibly for 1% of the series when imputation is not possible.

#### Moving Average Model (MA)

- **Definition**: Statistical model predicting the next value as a linear combination of past forecast errors. Captures short-term dependencies.
- Key Formula: xt = µ + Σ(θi * ϵt-i) + ϵt
- µ: Mean of the series; ϵt: White noise at time t; θi: Parameters; q: Order of the model.
- Applications: Smoothing noisy time series data; short-term prediction in finance and weather.

#### Auto Regressive Model (AR)

- **Definition**: Model predicting the current value as a linear combination of its past values. Captures influence of previous time steps.
- Key Formula: xt = c + Σ(φi * xt-i) + ϵt
- c: Constant term; φi: Parameters; ϵt: White noise at time t; p: Order of the model.
- Applications: Modeling and forecasting time series with strong temporal dependencies (economics, finance, weather prediction).

### 3.2 Time Series Imputation

- **Definition**: Filling in missing values within a time series.

#### Methods for Replacing Missing Values

- **Replacing with Mean**:
  - Method: Compute mean µ of observed values, replace each missing value with µ.
  - Advantages: Simple, computationally efficient, preserves overall average.
  - Limitations: May not capture temporal structure, can distort patterns if missing values are frequent.
- **Replacing with Median**:
  - Compute the median of observed values.
  - Less sensitive to outliers compared to the mean; suitable for skewed distributions.
- **Forward Fill (ffill)**: Replace missing values with the last observed value.
- **Backward Fill (bfill)**: Replace missing values with the next observed value.
- Advantages (ffill/bfill): Preserves local trends and continuity. Useful in real-time data streams and sensor readings.

### 3.3 Time Series Classification

- **Definition**: Assigning a category or label to a time series based on its patterns or features.

#### K-Nearest Neighbors (KNN) with Euclidean Distance (ED)

- Compares the distance between time series to find the closest match.
- Limitations: Requires series of equal length; insensitive to temporal shifting or frequency variations.

#### Dynamic Time Warping (DTW)

- Aligns two time series with shifting and scaling to measure similarity.
- Overcomes the limitations of KNN with ED by handling temporal distortions.
- Applications: Activity recognition, fault detection, medical diagnostics.
- Note (from handwritten source): classification a valeur Continue (classification with continuous value) refers to predicting a continuous label, which is covered under Extrinsic Regression.

### 3.4 Handling Unequal Length Series

- **Challenges**: Time series often have different lengths due to varying sampling durations or missing data.
- **Methods**:
  - **Padding**: Extend shorter series with zeros or other placeholder values.
  - **Truncation**: Trim longer series to match the shortest series length.
  - **Dynamic Time Warping (DTW)**: Aligns time series of unequal length by allowing non-linear mappings of time indices, preserving important temporal patterns.

### 3.5 Dimensionality Reduction/Symbolization of Time Series Data

#### Dimensionality Reduction

- **Principal Component Analysis (PCA)**: Can reduce dimensions, but does not account for temporal ordering.
- **Piecewise Aggregate Approximation (PAA)**: Reduces dimensionality by dividing the time series into equal-sized segments and averaging values within each segment.

#### Symbolization

- **Symbolic Aggregate approXimation (SAX)**: Converts PAA segments into symbols for simplified representation.
- **Symbolic Fourier Approximation (SFA)**: Utilizes Fourier Transform coefficients for symbolization.

## Conclusion (Key Takeaways)

- Time series are temporally ordered datasets crucial in fields like healthcare, finance, and engineering, characterized by temporal dependency, seasonality, and trends.
- They can be Univariate (single variable) or Multivariate (multiple variables with dependencies).
- Key techniques include:
  - Decomposition into components (trend, seasonality, cyclic patterns, noise).
  - Statistical features (mean, standard deviation, Z-normalization, auto-correlation, Fourier Transform, catch22).
  - Models like AR and MA for forecasting.
  - Imputation methods for missing data (mean, median, forward/backward fill).
  - Classification (KNN, DTW) and Extrinsic Regression (KNN-DTW, Random Forest, SVR, XGBoost).
  - Methods for handling unequal length series (padding, truncation, DTW).
  - Dimensionality reduction (PAA, PCA) and symbolization (SAX, SFA).""",
    "wooclap/1.md": """# Data Mining & Machine Learning Concepts

These notes provide a concise overview of fundamental concepts in data mining, machine learning algorithms (K-NN, Decision Trees, Bayes Classifier, Neural Networks), and clustering techniques (Hierarchical Clustering, K-Means), along with an introduction to Time Series analysis. They cover primary objectives, data types, preprocessing steps, algorithm characteristics, advantages, disadvantages, and key terminology.

## Intro (Data Mining Fundamentals)

- Primary objective of data mining: Discover patterns.
- Structured data example: A CSV file.
- Main purpose of data preprocessing: Clean data.
- Focus of exploratory data analysis (EDA): Understand data.
- Method for handling missing values by replacing them: Mean imputation.
- Technique for reducing data dimensionality: Principal Component Analysis (PCA).
- Language for managing relational databases: SQL.
- Type of unsupervised learning: Clustering.
- Purpose of big data analysis: Extract insights.
- Step involving assessing model quality: Evaluation.
- Key characteristic of unstructured data: Lacks a specific form.
- Discipline data mining intersects with for predictive analytics: Machine learning.
- "Veracity" in big data context: Data quality.
- Data visualization technique for showing variable distribution: Histogram.

## K-Nearest Neighbors (K-NN)

- Type of learning: Instance-based learning.
- Commonly used distance metric: Euclidean Distance.
- Primary disadvantage: High computational cost.
- Effect of small 'k': Sensitive to noise.
- Technique to ensure features contribute equally: Feature scaling.
- Type of voting assigning more influence to closer neighbors: Weighted voting.
- 'Lazy learning': No model building during training.
- Main purpose of one-hot encoding: Handle categorical features.
- Method for choosing optimal 'k': Cross-validation.
- Real-world application: Medical diagnosis.
- Key challenge with large datasets: High computational complexity.
- Scenario for weighted voting: When closer neighbors should have more influence.
- Critical preprocessing step for varying scales: Normalization.
- Strategy to avoid ties in binary classification: Choose an odd value for 'k'.

## Decision Trees

- Primary use: Classification and regression.
- Each internal node represents: A test on an attribute.
- Algorithm building trees using information gain: ID3 (Iterative Dichotomiser 3).
- 'Entropy': A measure of uncertainty.
- Overfitting: Learning the training data too well, including noise.
- Pruning process aim: Improve generalization by reducing complexity.
- Strategy to avoid overfitting by penalizing complexity: Cost Complexity Pruning (CCP).
- Common criterion to split nodes: Information gain.
- Random Forest's role in reducing overfitting: By averaging predictions of multiple trees.
- Data types handled effectively: Both numerical and categorical data.
- 'Minimum Description Length' principle in pruning: Prefer the simplest model that fits the data.
- 'Entropy gain': Reduction in uncertainty.
- Method for handling numerical data: Discretization.
- Problem from unrestricted tree growth: Overfitting.
- Why interpretable: Easy to understand and visualize.

## Bayes Classifier

- Bayes' Theorem used for: To calculate conditional probabilities.
- Naive Bayesian Classifier assumption about attributes: Independence.
- P(A): The probability of an event A.
- P(A ∩ B) for independent events: P(A)×P(B).
- P(A | B) represents: The probability of A given B.
- hMAP in Bayesian classification: Maximum A Posteriori Hypothesis.
- Attribute in text classification: A word in the text.
- Problem if feature value never appears in training set: Zero probability.
- Data handled well: Small datasets.
- Scenario not well suited for: Complex models with feature interactions.
- P(Ck) in Bayesian classification: The prior probability of class Ck.
- Method for very small probability values: Logarithmic transformation.
- Gaussian Naive Bayes assumption: Gaussian (Normal) distribution of attributes.
- Disadvantage of Naive Bayesian Classifier: Assumes conditional independence of features.
- Example of text classification: Spam email filtering.

## Hierarchical Clustering

- Primary goal of clustering: Divide a set of objects into groups.
- Clustering task type: Unsupervised.
- Common distance measure for numerical values: Euclidean distance.
- Purpose of scaling attributes: To ensure each attribute contributes equally.
- Dendrogram represents: Hierarchical representation of successive merges.
- Linkage method sensitive to noise: Single linkage.
- Main advantage of Ward's linkage: Creates balanced clusters.
- Determining number of clusters: Cut the tree at a specific height.
- Silhouette Coefficient type of evaluation criterion: Internal.
- Adjusted Rand Index compares: Clustering results to true labels.
- Major disadvantage: Computational cost of the distance matrix.
- Initial cluster formation: Each object is its own cluster.
- Centroid linkage method known for: Good resistance to noise.
- Number of clusters during process: Decreases over time.
- Criterion based on inertia: Ward's linkage.

## K-Means

- Primary objective: Minimize intra-cluster variance.
- First step: Choose k random points as cluster centers.
- Centroid update method: By calculating the mean of the points in each cluster.
- K represents: Number of clusters.
- Technique for improved centroid initialization: K-Means++.
- Convergence: When centroids no longer change.
- Common distance metric: Euclidean distance.
- Issue due to outliers: It distorts centroids.
- Method to decide 'k': Elbow Method.
- Common limitation: Assumes spherical clusters.
- Variant of K-Means: K-Medoids.
- Type of learning: Unsupervised learning.
- Silhouette Score's role: By evaluating the quality of clustering.
- Effect if 'k' is too large: Clusters may overlap too much.
- Main reason to use K-Means++: To improve initial centroid selection.

## Neural Networks

- Perceptron developed in 1960s by: Rosenblatt.
- Main limitation of perceptrons: Cannot solve non-linear problems.
- Backpropagation algorithm proposed in 1974 by: Werbos.
- Initial activation function in perceptrons: Heaviside.
- Key feature of LeNet-5: Designed for grayscale images.
- Distinguishes AlexNet from LeNet-5: Handles RGB images.
- Model introducing parallel filter sizes: Inception.
- VGG-16 learnable parameters: 138 million.
- Foundational principle to neural network training: Error correction.
- Inception technique to reduce computational cost: 1×1 convolutions.
- Common activation function in AlexNet: ReLU.
- Major achievement of AlexNet in 2012: Popularized deep learning.
- Primary advantage of deep neural networks: Non-linear decision boundaries.
- Problem of non-linear decision boundaries highlighted by: Minsky and Papert.
- Output of a perceptron passed through: Activation function.

## Time Series

- Key property of time series data: Temporal order matters.
- Z-Normalization: Standardizes the series to mean 0 and standard deviation 1.
- Primary goal of time series forecasting: Predict future values based on past data.
- Technique aligning time series of unequal lengths: Dynamic Time Warping (DTW).
- Component representing long-term progression: Trend.
- Imputation method using last observed value: Forward Fill.
- Purpose of Fourier Transform: Decomposes the series into frequency components.
- Method best for capturing cyclic patterns: Fourier Transform.
- Method filling missing values with median of observed values: Forward Fill. (Note: This seems like a repeat or slight variation of "Forward Fill" answer, but input text states "median of observed values" for a method identified as Forward Fill.)
- Key advantage of DTW in classification: It handles temporal distortions.
- Statistical feature capturing spread: Variance.
- Model predicting future value based on weighted sum of past values: Auto Regressive (AR) model.""",
    "wooclap/qcm.md": """# QCM du Cours : Data Mining & Machine Learning

## Intro

* **What is the primary objective of data mining?**
* Discover patterns


* **Which of the following is an example of structured data?**
* A CSV file


* **What is the main purpose of data preprocessing in data mining?**
* Clean data


* **What is the focus of exploratory data analysis (EDA)?**
* Understand data


* **Which method is used for handling missing values by replacing them?**
* Mean imputation


* **What technique is commonly used for reducing data dimensionality?**
* Principal Component Analysis (PCA)


* **Which language is commonly used for managing relational databases?**
* SQL


* **Which of the following is a type of unsupervised learning?**
* Clustering


* **What is the purpose of big data analysis in data mining?**
* Extract insights


* **Which step in the data mining process involves assessing model quality?**
* Evaluation


* **What is the key characteristic of unstructured data?**
* Lacks a specific form


* **Which discipline does data mining heavily intersect with for predictive analytics?**
* Machine learning


* **What does the term "veracity" refer to in the context of big data?**
* Data quality


* **Which data visualization technique is ideal for showing the distribution of a variable?**
* Histogram



## KNN (k-Nearest Neighbors)

* **What type of learning is k-Nearest Neighbors (k-NN) classified under?**
* Instance-based learning


* **Which distance metric is commonly used in the k-NN algorithm?**
* Euclidean Distance


* **What is a primary disadvantage of the k-NN algorithm?**
* High computational cost


* **What is the effect of choosing a small value of 'k' in k-NN?**
* Sensitive to noise


* **Which technique is used to ensure features contribute equally to distance computation in k-NN?**
* Feature scaling


* **Which type of voting in k-NN assigns more influence to closer neighbors?**
* Weighted voting


* **What does 'lazy learning' refer to in the context of k-NN?**
* No model building during training


* **What is the main purpose of one-hot encoding in k-NN?**
* Handle categorical features


* **Which method helps in choosing an optimal value for 'k' in k-NN?**
* Cross-validation


* **What is a real-world application of the k-NN algorithm?**
* Medical diagnosis


* **What is a key challenge of using k-NN with large datasets?**
* High computational complexity


* **In which scenario is weighted voting particularly useful in k-NN?**
* When closer neighbors should have more influence


* **What preprocessing step is critical for k-NN when dealing with features of varying scales?**
* Normalization


* **What is a common strategy to avoid ties in k-NN binary classification problems?**
* Choose an odd value for 'k'



## Decision Trees

* **What is a Decision Tree primarily used for?**
* Classification and regression


* **What does each internal node of a Decision Tree represent?**
* A test on an attribute


* **Which algorithm is widely known for building Decision Trees using information gain?**
* ID3 (Iterative Dichotomiser 3)


* **What is 'entropy' in the context of Decision Trees?**
* A measure of uncertainty


* **What is overfitting in the context of Decision Trees?**
* Learning the training data too well, including noise


* **What does the pruning process in Decision Trees aim to achieve?**
* Improve generalization by reducing complexity


* **Which strategy is used to avoid overfitting by penalizing the complexity of the tree?**
* Cost Complexity Pruning (CCP)


* **In a Decision Tree, which criterion is commonly used to split nodes?**
* Information gain


* **How does a Random Forest help to reduce overfitting?**
* By averaging predictions of multiple trees


* **Which type of data can Decision Trees handle effectively?**
* Both numerical and categorical data


* **What is the 'Minimum Description Length' principle in pruning Decision Trees?**
* Prefer the simplest model that fits the data


* **What does 'entropy gain' represent in Decision Tree construction?**
* Reduction in uncertainty


* **Which method is used to handle numerical data in Decision Trees?**
* Discretization


* **Which problem arises from allowing a Decision Tree to grow without restriction?**
* Overfitting


* **Why are Decision Trees considered interpretable models?**
* Easy to understand and visualize



## Bayes Classifier

* **What is Bayes' Theorem used for?**
* To calculate conditional probabilities


* **What does the Naive Bayesian Classifier assume about attributes?**
* Independence


* **What symbol represents the probability of an event A?**
* 


* **If events A and B are independent, what is ?**
* 


* **What does  represent?**
* The probability of A given B


* **In Bayesian classification, what is the term ?**
* Maximum A Posteriori Hypothesis


* **In the context of text classification, what can an attribute represent?**
* A word in the text


* **What problem occurs if a feature value never appears in the training set?**
* Zero probability


* **What kind of data does the Bayesian Classifier handle well?**
* Small datasets


* **Which scenario is the Bayesian Classifier not well suited for?**
* Complex models with feature interactions


* **What does  represent in Bayesian classification?**
* The prior probability of class 


* **Which method is used to deal with very small probability values in Bayesian classification?**
* Logarithmic transformation


* **In Gaussian Naive Bayes, what is assumed about the distribution of attributes?**
* Gaussian (Normal) distribution


* **Which of the following is a disadvantage of the Naive Bayesian Classifier?**
* Assumes conditional independence of features


* **Which of the following is an example of text classification using Bayesian methods?**
* Spam email filtering



## Hierarchical Clustering

* **What is the primary goal of clustering?**
* Divide a set of objects into groups


* **What type of task is clustering considered?**
* Unsupervised


* **Which distance measure is commonly used for numerical values in clustering?**
* Euclidean distance


* **What is the purpose of scaling attributes in clustering?**
* To ensure each attribute contributes equally


* **What does a dendrogram represent in hierarchical clustering?**
* Hierarchical representation of successive merges


* **Which linkage method is sensitive to noise in hierarchical clustering?**
* Single linkage


* **What is the main advantage of Ward’s linkage method?**
* Creates balanced clusters


* **How do you determine the number of clusters in hierarchical clustering?**
* Cut the tree at a specific height


* **What type of evaluation criterion is the Silhouette Coefficient?**
* Internal


* **What does the Adjusted Rand Index compare?**
* Clustering results to true labels


* **What is a major disadvantage of hierarchical clustering?**
* Computational cost of the distance matrix


* **In hierarchical clustering, how are clusters formed initially?**
* Each object is its own cluster


* **What is the centroid linkage method known for?**
* Good resistance to noise


* **What happens to the number of clusters during the hierarchical clustering process?**
* Decreases over time


* **Which criterion is based on inertia in hierarchical clustering?**
* Ward's linkage



## KMeans

* **What is the primary objective of K-Means clustering?**
* Minimize intra-cluster variance


* **Which step comes first in the K-Means algorithm?**
* Choose k random points as cluster centers


* **How are cluster centers (centroids) updated in K-Means?**
* By calculating the mean of the points in each cluster


* **What is K in K-Means?**
* Number of clusters


* **What technique is used to initialize centroids to improve convergence in K-Means?**
* K-Means++


* **What does convergence in K-Means mean?**
* When centroids no longer change


* **Which distance metric is commonly used in K-Means?**
* Euclidean distance


* **What issue arises due to outliers in K-Means?**
* It distorts centroids


* **Which method helps to decide the number of clusters (k) in K-Means?**
* Elbow Method


* **What is a common limitation of K-Means?**
* Assumes spherical clusters


* **Which of the following is a variant of K-Means?**
* K-Medoids


* **What type of learning is K-Means clustering?**
* Unsupervised learning


* **How does the Silhouette Score help in K-Means clustering?**
* By evaluating the quality of clustering


* **What happens if k is too large in K-Means?**
* Clusters may overlap too much


* **What is the main reason to use K-Means++?**
* To improve initial centroid selection



## Neural Networks

* **Who developed the perceptron in the 1960s?**
* Rosenblatt


* **What is the main limitation of perceptrons?**
* Cannot solve non-linear problems


* **Who proposed the backpropagation algorithm in 1974?**
* Werbos


* **What activation function was initially used in perceptrons?**
* Heaviside


* **What is a key feature of LeNet-5?**
* Designed for grayscale images


* **What distinguishes AlexNet from LeNet-5?**
* Handles RGB images


* **Which model introduced the concept of parallel filter sizes?**
* Inception


* **How many learnable parameters does VGG-16 have?**
* 138 million


* **What principle is foundational to neural network training?**
* Error correction


* **What technique does Inception use to reduce computational cost?**
* 1×1 convolutions


* **What activation function is commonly used in AlexNet?**
* ReLU


* **What was a major achievement of AlexNet in 2012?**
* Popularized deep learning


* **What is the primary advantage of deep neural networks?**
* Non-linear decision boundaries


* **Who highlighted the problem of non-linear decision boundaries?**
* Minsky and Papert


* **What is the output of a perceptron passed through?**
* Activation function



## Time Series

* **What is the key property of time series data?**
* Temporal order matters


* **What does Z-Normalization do to a time series?**
* Standardizes the series to mean 0 and standard deviation 1


* **What is the primary goal of time series forecasting?**
* Predict future values based on past data


* **Which technique aligns time series of unequal lengths?**
* Dynamic Time Warping (DTW)


* **What component of time series represents long-term progression or direction?**
* Trend


* **Which imputation method fills missing values by using the last observed value?**
* Forward Fill


* **What is the purpose of the Fourier Transform in time series?**
* Decomposes the series into frequency components


* **Which method is best for capturing cyclic patterns in time series?**
* Fourier Transform


* **Which method is used for filling missing time series data by replacing each missing value with the median of the observed values?**
* Forward Fill


* **What is the key advantage of Dynamic Time Warping (DTW) in time series classification?**
* It handles temporal distortions


* **What statistical feature captures the spread or dispersion of a time series?**
* Variance


* **Which model is often used to predict the future value based on the weighted sum of past values in time series analysis?**
* Auto Regressive (AR) model"""
}


def strip_markdown(text):
    """Strip basic markdown syntax from text."""
    import re
    # Remove headers
    text = re.sub(r"^#+\s*", "", text, flags=re.MULTILINE)
    # Remove links
    text = re.sub(r"\[([^\]]+)\]\([^\)]+\)", r"\1", text)
    # Remove bold
    text = re.sub(r"\*\*([^\*]+)\*\*", r"\1", text)
    # Remove italic
    text = re.sub(r"\*([^\*]+)\*", r"\1", text)
    # Remove code blocks
    text = re.sub(r"```[\s\S]*?```", "", text)
    # Remove inline code
    text = re.sub(r"`([^`]+)`", r"\1", text)
    # Remove lists
    text = re.sub(r"^[\s]*[-\*\+]\s*", "", text, flags=re.MULTILINE)
    return text


def display_text(text, max_lines=20):
    """Display text in chunks for limited screen."""
    lines = text.split("\n")
    for i in range(0, len(lines), max_lines):
        chunk = "\n".join(lines[i : i + max_lines])
        print(chunk)
        if i + max_lines < len(lines):
            input("Press Enter to continue...")


def list_courses():
    """List all available courses from embedded content."""
    files = sorted(EMBEDDED_CONTENT.keys())
    return files


def view_courses():
    """View all courses by listing and displaying embedded content."""
    files = list_courses()
    if not files:
        print("No courses found.")
        return
    print("Available courses:")
    for i, file in enumerate(files):
        print(f"{i+1}. {file}")
    choice = input("Enter number to view (or 'q' to quit): ")
    if choice.lower() == "q":
        return
    try:
        idx = int(choice) - 1
        if 0 <= idx < len(files):
            content = EMBEDDED_CONTENT[files[idx]]
            stripped = strip_markdown(content)
            display_text(stripped)
        else:
            print("Invalid choice.")
    except ValueError:
        print("Invalid input.")


def search_courses(query):
    """Search for query in all embedded content."""
    files = list_courses()
    results = []
    for file in files:
        content = EMBEDDED_CONTENT[file]
        if query.lower() in content.lower():
            results.append(file)
    if not results:
        print("No matches found.")
        return
    print("Matching files:")
    for i, file in enumerate(results):
        print(f"{i+1}. {file}")
    choice = input("Enter number to view (or 'q' to quit): ")
    if choice.lower() == "q":
        return
    try:
        idx = int(choice) - 1
        if 0 <= idx < len(results):
            content = EMBEDDED_CONTENT[results[idx]]
            stripped = strip_markdown(content)
            display_text(stripped)
        else:
            print("Invalid choice.")
    except ValueError:
        print("Invalid input.")


def bayes_calculator():
    """Interactive Bayes theorem calculator."""
    print("Bayes Calculator")
    try:
        prior = float(input("Enter prior probability P(H): "))
        likelihood = float(input("Enter likelihood P(E|H): "))
        evidence = float(input("Enter evidence P(E): "))
        posterior = (prior * likelihood) / evidence
        print(f"Posterior probability P(H|E): {posterior}")
    except ValueError:
        print("Invalid input. Please enter numbers.")


def euclidean_distance_calculator():
    """Interactive Euclidean distance calculator."""
    print("Euclidean Distance Calculator")
    try:
        n = int(input("Enter number of dimensions: "))
        point1 = []
        point2 = []
        for i in range(n):
            p1 = float(input(f"Enter coordinate {i+1} for point 1: "))
            p2 = float(input(f"Enter coordinate {i+1} for point 2: "))
            point1.append(p1)
            point2.append(p2)
        distance = sum((a - b) ** 2 for a, b in zip(point1, point2)) ** 0.5
        print(f"Euclidean distance: {distance}")
    except ValueError:
        print("Invalid input. Please enter numbers.")


def entropy_calculator():
    """Interactive entropy calculator."""
    print("Entropy Calculator")
    try:
        probs = []
        while True:
            prob = input("Enter probability (or 'done'): ")
            if prob.lower() == "done":
                break
            probs.append(float(prob))
        if not probs:
            print("No probabilities entered.")
            return
        import math
        entropy = -sum(p * math.log2(p) for p in probs if p > 0)
        print(f"Entropy: {entropy}")
    except ValueError:
        print("Invalid input. Please enter numbers.")


def execute_math():
    """Simple math expression evaluator."""
    print("Math Executor")
    expr = input("Enter math expression: ")
    try:
        result = eval(expr)
        print(f"Result: {result}")
    except:
        print("Invalid expression.")


def main():
    """Main menu."""
    while True:
        print("\nData Mining Course Viewer and Calculator")
        print("1. View Courses")
        print("2. Search Courses")
        print("3. Bayes Calculator")
        print("4. Euclidean Distance Calculator")
        print("5. Entropy Calculator")
        print("6. Execute Math")
        print("7. Quit")
        choice = input("Choose an option: ")
        if choice == "1":
            view_courses()
        elif choice == "2":
            query = input("Enter search query: ")
            search_courses(query)
        elif choice == "3":
            bayes_calculator()
        elif choice == "4":
            euclidean_distance_calculator()
        elif choice == "5":
            entropy_calculator()
        elif choice == "6":
            execute_math()
        elif choice == "7":
            break
        else:
            print("Invalid choice.")


if __name__ == "__main__":
    main()
