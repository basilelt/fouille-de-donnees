# Notes sur le Clustering Hiérarchique

Le clustering hiérarchique est une tâche d'apprentissage non supervisé qui vise à construire des groupes (clusters) d'objets similaires, représentés par une structure arborescente (dendrogramme). Il se base sur des mesures de dissimilarité entre objets et la manière de calculer la distance entre les clusters, ce qui impacte fortement les résultats.

## 1. Introduction au Clustering

- **Définition**: Le clustering est une tâche non supervisée, contrairement à la classification qui est supervisée et utilise des données labellisées.
- **Objectif**: Construire des groupes d'objets (clusters) à partir de leur description, sans labels préexistants.
- **Principes**:
  - Maximiser la similarité intracluster (objets du même groupe doivent être les plus proches possible).
  - Minimiser la similarité intercluster (objets de groupes différents doivent être les plus éloignés possible).

## 2. Mesure de Dissimilarité (Distance)

- **Concept**: Définir une mesure pour évaluer de manière graduelle le degré de ressemblance entre deux objets.
- **Propriétés**: Pour être appelée "distance", cette mesure doit satisfaire certaines propriétés mathématiques.
- **Exemple (Clients)**: Marie, Bruno, Laurent décrits par l'âge et le salaire.
- **Visualisation dans un espace à deux dimensions**.
- **Distance Euclidienne**:
  - Souvent utilisée pour des vecteurs de valeurs numériques.
  - Calculée comme la racine carrée de la somme des différences au carré des caractéristiques.
- **Normalisation des Caractéristiques**:
  - Nécessaire avant le calcul de la distance (en particulier Euclidienne) pour éviter qu'une caractéristique avec une plus grande amplitude ne domine le calcul.
  - Objectif: Assurer que chaque caractéristique ait le même poids dans le calcul de la distance.
  - **Méthode: Normalisation Min-Max**:
    - Formule: $(valeur - min) / (max - min)$
    - Ramène toutes les caractéristiques dans le même domaine de définition (généralement entre 0 et 1).
- **Matrice de Distance**:
  - Matrice 2x2 qui stocke la distance entre chaque couple d'objets.
  - Complexité quadratique par rapport au nombre d'objets.
- **Choix de la Distance**: Dépend du type de données (ex: numérique, images, vidéos, sons, documents, utilisateurs). Il est crucial de définir une distance pertinente pour les objets à regrouper.

## 3. Algorithme de Clustering Hiérarchique Ascendant (Agglomératif)

- **Processus**:
  1. **Initialisation**: Chaque objet est un cluster unique (ex: 100 objets = 100 clusters).
  2. **Itération**:
     - Calculer la matrice de distance 2 à 2 entre tous les clusters.
     - Sélectionner les deux clusters les plus proches (distance minimale).
     - Fusionner ces deux clusters en un seul.
  3. **Terminaison**: Répéter jusqu'à ce que tous les objets soient dans un seul groupe (un seul cluster).
- **Dendrogramme**:
  - Représentation hiérarchique des fusions successives.
  - La hauteur des liens est proportionnelle à la distance à laquelle les clusters ont été fusionnés.
  - **Découpe**: Permet d'obtenir une partition en un nombre désiré de clusters en "coupant" le dendrogramme à une certaine hauteur. La forme du dendrogramme peut donner une indication sur le nombre de clusters.
- **Critère de Lien (Linkage) - Mesure de la distance entre groupes**: C'est un paramètre clé qui définit comment la distance entre deux clusters est calculée.
  - **Distance Minimum (Single Linkage)**: Distance entre les deux objets les plus proches de chaque cluster. Tendance à l'effet de "chaînage".
  - **Distance Maximum (Complete Linkage)**: Distance entre les deux objets les plus éloignés de chaque cluster.
  - **Distance Moyenne (Average Linkage)**: Moyenne de toutes les distances possibles entre paires d'objets des deux clusters. Coûteux.
  - **Centroïde**: Distance entre les centroïdes (moyennes) des clusters.
  - **Critère de Ward**: Basé sur l'inertie, tend à créer des clusters de cardinalités (nombre d'objets) similaires.
- Le choix du critère de lien entraîne des résultats de clustering différents même avec les mêmes données.

## 4. Exemples d'Utilisation

- **Contrainte**: La complexité quadratique de la matrice de distance limite son application aux grands volumes de données. Utilisé quand le nombre d'objets est "acceptable".
- **Applications**:
  - Bioinformatique: Regroupement de données d'expression de gènes.
  - Neurosciences: Clustering de représentations d'images issues d'IRM cérébrales pour étudier la représentation abstraite et visuelle.
  - Analyse comportementale: Clustering de séquences d'activités de chirurgiens (seniors vs. juniors) pour identifier des schémas opératoires.

## 5. Évaluation du Clustering

- **Différence avec la classification**: Pas toujours de vérité terrain (labels de classe) pour vérifier la qualité des clusters.
- **Deux approches principales**:
  - **Critères Internes**: Basés uniquement sur la qualité intrinsèque des clusters.
    - Mesurent la compacité (objets proches au sein d'un cluster) et la séparation (clusters distincts les uns des autres) dans l'espace des données.
    - Exemples: Coefficient de Silhouette, Dunn Index.
  - **Critères Externes**: Comparent la partition obtenue par l'algorithme à un clustering ou à des classes préexistantes (si disponibles).
    - Gèrent l'absence de correspondance directe entre les numéros de cluster (ex: Cluster 1) et les noms de classes (ex: Iris setosa), ainsi que la possibilité d'un nombre de clusters différent.
    - Exemples: Rand Index, Adjusted Rand Index.

## 6. Conclusion : Avantages et Inconvénients du Clustering Hiérarchique Ascendant

- **Avantages**:
  - **Simplicité et Intuitivité**: Facile à comprendre (processus itératif de regroupement).
  - **Flexibilité du nombre de classes**: Le nombre de clusters n'est pas un paramètre initial; il est choisi après la construction du dendrogramme par une simple coupe.
  - **Visualisation intuitive**: Le dendrogramme aide à orienter le choix du nombre de clusters.
- **Inconvénients**:
  - **Coût de la matrice de distance**: La complexité quadratique $O(n^2)$ (où n est le nombre d'objets) rend l'algorithme inadapté aux très grands volumes de données.
  - **Décisions irréversibles**: Les fusions de clusters ne sont jamais remises en question au cours de l'algorithme (approche gloutonne).
  - **Variabilité**: Les résultats dépendent fortement du critère de lien choisi (single, complete, average, Ward, etc.), ce qui peut rendre difficile le choix du "meilleur" critère.