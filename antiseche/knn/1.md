# Le Plus Proche Voisin (k-NN)

## Main Takeaway
Le k-Plus Proche Voisin (k-NN) est un algorithme d'apprentissage automatique basé sur des instances (lazy learning) qui permet de faire de la classification et de la régression en se basant sur la classe majoritaire ou la moyenne des k objets les plus proches dans l'ensemble d'entraînement. Simple à comprendre et à mettre en œuvre, il est adaptable mais présente des défis majeurs en termes de coût computationnel pour de grands volumes de données et nécessite une préparation rigoureuse des données, notamment la normalisation des caractéristiques et le choix optimal du paramètre k.

## Introduction
- Algorithme basé sur des instances (instance-based learning ou lazy learning).
- Permet de faire de la classification (prédire valeur discrète) et de la régression (prédire valeur continue). L'accent est mis sur la classification.
- Fonctionnement : Classifie un nouvel objet en se basant sur la classe majoritaire au sein des k plus proches voisins.
- k est le nombre de voisins à considérer et est un paramètre de l'algorithme.
- 1-Plus Proche Voisin (1-NN) : Cas particulier où on cherche uniquement l'objet le plus proche.
- Le k-NN est une famille d'algorithmes avec de nombreuses versions selon le choix de la distance, de k, de la pondération, etc.

## Concepts Clés

### Mesure de Distance
- Nécessaire pour définir la notion de "proximité".
- Distance Euclidienne : Très classique et largement utilisée pour les vecteurs de valeurs numériques.
- Autres distances : Manhattan, Minkowski, similarité cosinus. Le choix dépend du type de données et des caractéristiques souhaitées.

### Apprentissage Paresseux (Lazy Learning)
- Dit "paresseux" car il n'y a pas d'étape de construction de modèle à l'avance.
- Le "modèle" est constitué par l'intégralité des données d'entraînement.
- Lors de l'inférence (classification d'un nouvel objet), il est comparé à tous les objets du jeu d'entraînement.

## Historique et Applications
- Développé dans les années 1950.
- Formalisé par Cover et Hart en 1967.
- Applications courantes :
  - Vision par ordinateur / Reconnaissance d'images.
  - Systèmes de recommandation.
  - Médecine / Diagnostic médical (facilement explicable).
  - Recherche d'information.
- Implémenté dans l'intégralité des librairies d'apprentissage automatique.

## Avantages
- Simplicité : Facile à comprendre et à expliquer (ex: "chercher le plus proche").
- Polyvalence : Utilisable pour la classification et la régression.
- Rapidité de mise en place : Pas de phase de "training" coûteuse en temps de construction de modèle.
- Non-paramétrique : Ne fait pas d'hypothèses sur la distribution des données, peut construire des frontières de classe complexes.
- Mise à jour facile : Les nouvelles données sont simplement ajoutées au jeu d'entraînement et prises en compte instantanément pour les classifications futures, sans reconstruire de modèle.
- Explicabilité : Facile d'expliquer une prédiction en montrant les voisins les plus proches (particulièrement utile en diagnostic médical).

## Inconvénients et Défis

### Coût Algorithmique Élevé (Temps et Mémoire)
- Problématique pour les gros volumes de données ou un grand nombre de caractéristiques.
- Chaque classification implique la recherche des k plus proches voisins dans tout l'ensemble d'entraînement.
- Le calcul de distance peut être coûteux pour des données complexes (images, texte).

### Sensibilité au Bruit
- Un point bruité ou outlier peut fortement influencer la classification des objets à proximité.
- Une petite valeur de k augmente cette sensibilité.

### Choix de k Difficile
- La valeur de k a un impact significatif sur les performances.
- Petit k : Sensible au bruit, risque de surapprentissage (overfitting), frontières de classification complexes.
- Grand k : Plus résistant au bruit, risque de sous-apprentissage (underfitting), frontières de décision plus lisses.
- Méthodes pour choisir k : Essai-erreur, validation croisée (analyse du taux d'erreur).
- Pour les données binaires, choisir un k impair (ex: 3 ou 5) aide à éviter les égalités de vote.

### Nécessité de Normalisation des Caractéristiques
- Indispensable pour que toutes les caractéristiques aient le même poids dans le calcul de distance.

## Exemple Visuel (Données IRIS)
- Utilisation des données IRIS : 150 objets (50 Setosa, 50 Vericolor, 50 Virginica).
- Un jeu d'entraînement et un jeu de test sont définis.
- Pour chaque objet du jeu de test, on cherche le plus proche voisin (ici, en 1-NN) dans le jeu d'entraînement pour assigner sa classe.

## Implémentation Naïve et Optimisations
- Fonction de classification naïve (1-NN) :

```
Pour chaque objet_test à classer:
    Trouver dans l'ensemble d'entraînement (train) l'objet_train le plus proche.
    Assigner la classe de l'objet_train trouvé à l'objet_test.
```

- Optimisations : Techniques pour accélérer le processus de recherche des voisins (ex: calcul de lower bound, pruning, mise en cache de mesures de distance).

## Pondération des Voisins
- Version de base : Vote à la majorité simple (tous les voisins ont le même poids).
- Alternative (pondération par distance) :
  - Attribuer un poids proportionnel à la distance de chaque voisin. Les voisins plus proches ont un poids plus important.
  - Utile pour résoudre les égalités de vote et donner plus de pertinence aux objets très proches.
  - Exemple de formule : \( \frac{1}{\text{distance}^2} \) (ou d'autres formules).

## Normalisation des Données (Mise à l'Échelle)
- Principe fondamental : Systématiquement mettre à l'échelle ou normaliser les données avant tout calcul de distance.
- Objectif : Ramener les caractéristiques à une même échelle (ex: entre 0 et 1).
- Exemple : La normalisation Min-Max est courante :

\[
\text{Valeur_normalisée} = \frac{\text{Valeur_originale} - \text{Min_caractéristique}}{\text{Max_caractéristique} - \text{Min_caractéristique}}
\]

- Raison : Sans normalisation, les caractéristiques avec de plus grandes échelles (ex: prix vs. taille en mètres carrés) domineraient le calcul de distance, indépendamment de leur importance réelle.

## Données Catégorielles
- Problème : Les mesures de distance standards sont conçues pour des valeurs numériques.
- Solution : One-Hot Encoding (encodage binaire).
- Convertit une caractéristique catégorielle en un vecteur numérique binaire.
- Ex: Une caractéristique "Couleur" avec les valeurs "red", "green", "blue" serait encodée en :
  - "red" -> [1, 0, 0]
  - "green" -> [0, 1, 0]
  - "blue" -> [0, 0, 1]
- Inconvénient : Peut augmenter considérablement le nombre de colonnes si une caractéristique prend beaucoup de valeurs possibles, ce qui impacte la complexité.

## Résumé et Points Clés
- Algorithme adaptable pour classification, régression, recommandation.
- Puissant et simple à expliquer, mais demande un travail significatif sur les données.
- Le choix de la mesure de distance est crucial et dépend du type de données.
- La complexité temporelle est le défi majeur avec de grands datasets.
- Le k-NN est une famille d'algorithmes avec de nombreuses variantes.
- Importance du prétraitement des caractéristiques : mise à l'échelle (normalisation) et gestion des données manquantes.
- Le paramètre k est central et doit être réglé avec soin.
- Avantages notoires : Explicabilité (ex: diagnostic médical) et facilité de mise à jour des données sans reconstruction du modèle.