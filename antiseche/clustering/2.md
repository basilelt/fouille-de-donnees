# Data Mining - Hierarchical Clustering

**Presented by Germain Forestier, PhD, Universit√© de Haute-Alsace**

## Summary/Main Takeaway

Hierarchical clustering is an unsupervised data mining technique that organizes a set of objects into a hierarchy of clusters, often visualized as a dendrogram. It involves iteratively merging or splitting clusters based on defined dissimilarity measures and linkage criteria. While it provides intuitive visualization and flexibility in determining the number of clusters, it can be computationally intensive and results may vary depending on the chosen distance and linkage methods.

## 1. Introduction to Clustering

### Objectives:
- Divide a set of objects into groups (clusters).
- Maximize intra-cluster similarity and minimize inter-cluster similarity.
- Define a measure of dissimilarity (or distance) between objects.
- Clustering is an unsupervised task.

### Dissimilarity between Objects:
- Allows gradual evaluation of resemblance.
- Must satisfy mathematical properties to be called a distance.

#### For numerical values:
- **Euclidean distance:** $\sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$
- Requires scaling of attributes (e.g., Age vs. Salary).

**Scaling techniques:**
- **MinMax scaling:** $z = \frac{x - \min(x)}{\max(x) - \min(x)}$
- **Example:** Comparing clients (Marie, Bruno, Laurent) using scaled Age and Salary. $d(\text{Marie}, \text{Bruno}) \approx 1.22$.

- Other distances exist for numerical values.

#### For other types of data:
- Specific distances for each data type.
- Objects can be described by multiple data types (e.g., image, video, sound, document, user).

## 2. Hierarchical Clustering Method

### Method Steps:
1. Start with one cluster per object.
2. At each step, two clusters are merged.
3. Cut the tree to obtain a partition.
4. Calculate the pairwise distance matrix between objects.
5. Select the smallest value in this matrix to identify the most similar groups.
6. Merge these two groups, replacing their rows with a row for the newly created group (based on a chosen criterion).
7. Repeat until a single group containing all objects is obtained.
8. This creates a sequence of partitions where the number $k$ of groups varies from $m$ (number of objects) to 1.
9. Finally, choose the value of $k$ using various possible methods.

### Dendrogram:
- A hierarchical representation of successive merges.
- The height of a link is proportional to the distance at which clusters were merged.

### Defining the Distance Between Clusters (Linkage Criteria):
1. **Minimum jump (single linkage):** Sensitive to noise.
2. **Maximum jump (complete linkage):** Tends to produce specific clusters; sensitive to noisy individuals.
3. **Average jump (average linkage):** Tends to produce clusters with similar variance.
4. **Centroid:** Good resistance to noise; not always possible to calculate.
5. **Ward:** Aggregation criterion based on inertia; creates "balanced" clusters.

**Note:** Different linkage criteria can lead to significantly different clustering results.

## 3. Example of Usage
- Clustering of gene expression data (Eisen et al., 1998).
- Clustering of similar image representation (Mur et al., 2013).
- Study of surgeon behavior (Forestier et al., 2012).

## 4. Clustering Evaluation

### Internal Evaluation:
- Evaluates the quality of clusters based solely on the data itself, without external labels.
- Focuses on compactness and separation of clusters.

#### Examples of Criteria:
- **Silhouette Coefficient:** $s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}$
  - $a(i)$: average distance between point $i$ and all other points in the same cluster.
  - $b(i)$: minimum average distance between point $i$ and points in another cluster.

- **Dunn Index:** $D = \frac{\min_{1 \leq i < j \leq k} \delta(C_i, C_j)}{\max_{1 \leq l \leq k} \Delta(C_l)}$
  - $\delta(C_i, C_j)$: distance between clusters $C_i$ and $C_j$.
  - $\Delta(C_l)$: diameter of cluster $C_l$.

### External Evaluation:
- Compares clustering results to a ground truth or external labels.
- Measures how well the clustering matches the true classification.

#### Examples of Criteria:
- **Rand Index (RI):** $RI = \frac{TP + TN}{TP + FP + FN + TN}$
  - $TP$: True Positives; $TN$: True Negatives.
  - $FP$: False Positives; $FN$: False Negatives.

- **Adjusted Rand Index (ARI):** $ARI = \frac{RI - E[RI]}{\max(RI) - E[RI]}$
  - $E[RI]$: Expected Rand Index of random clustering.

## 5. Conclusion

### Advantages:
- Easy to understand.
- Allows easy variation in the number of clusters.
- Intuitive visualization (dendrogram).

### Disadvantages:
- Computational cost of the distance matrix.
- The clustering criterion depends on the groups already formed.
- Different results depending on the clustering criterion (linkage method).