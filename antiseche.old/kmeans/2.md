# K-Means Clustering: Comprehensive Notes

## Main Takeaway
K-Means clustering is a widely used unsupervised learning technique that partitions data into k distinct, non-overlapping groups (clusters) based on similarity. Its primary objective is to minimize intra-cluster variance, thereby enhancing homogeneity within clusters. The algorithm operates iteratively, assigning data points to the nearest cluster centroid and then recomputing these centroids, continuing until a stopping criterion is met. While valued for its simplicity and efficiency, especially with large datasets, K-Means requires the number of clusters (k) to be predefined and is notably sensitive to the initial selection of centroids and the presence of outliers.

## 1. Introduction to K-Means Clustering
- **Source:** Germain Forestier, PhD, Université de Haute-Alsace.
- **Clustering:**
  - A technique in unsupervised learning.
  - Groups data based on similarity.
- **Partitioning Clustering:**
  - Divides the dataset into distinct, non-overlapping groups.
- **K-Means Clustering:**
  - A widely-used method where k specifies the number of clusters.
  - Objective: Minimize intra-cluster variance to enhance homogeneity.
  - Significance: Known for its simplicity and efficiency, especially in large datasets.

## 2. Understanding the K-Means Algorithm

### Overview
- Iteratively constructs and refines partitions to form satisfactory clusters.
- Involves calculating the mean to determine cluster centers (centroids).

### Steps of the K-Means Algorithm
1. Select k clusters to form.
2. Initialize by choosing k random points as cluster centers.
3. Assign each point to the nearest cluster center.
4. Recompute the centroids for each cluster by calculating the mean of all points assigned to that cluster.
5. Repeat the assignment and update steps until the stopping criterion is met (e.g., no points change clusters, a fixed number of iterations like 10-15 iterations).

### Exercise Example: E-commerce Customer Study
- **Objective:** Apply K-Means to study customers based on 'Number of Visits' and 'Number of Purchases'.
- **Data:** 10 clients with 2 characteristics (x, y) representing visits and purchases.
- **Initial Centers for 3 Clusters:**
  - Cluster 1: (1.5, 3.0)
  - Cluster 2: (4.0, 0.5)
  - Cluster 3: (2.5, 5.0)
- **Example Cluster Means (after initial iteration):**
  - Cluster 1: (1.50, 1.50)
  - Cluster 2: (4.66, 2.33)
  - Cluster 3: (5.33, 5.66)

## 3. Application Examples
- **Histopathological Image Processing:**
  - Goal: Automatically identify different structures in images captured with an electron microscope.
  - The colors in the image correspond to various structures.
  - Clustering results shown for 2, 3, and 4 clusters.

## 4. Fundamentals of K-Means

### Choosing 'k' in K-Means
- **Importance:** Directly impacts clustering quality; balances sensitivity to noise/outliers.
- **Methods for Determining 'k':**
  - Elbow Method: Identify the 'elbow' point in the plot of squared distances.
  - Silhouette Score: Higher scores indicate better-defined clusters.
  - Cross-validation: Assess cluster stability and predictive strength.
  - Practical Tips: Use domain knowledge; test various k values and evaluate.

### Distance Metrics in K-Means
- **Role:** Define similarity between objects; influence cluster shape and size.
- **Common Distance Metrics:**
  - Euclidean Distance: Straight line distance in space.
  - Manhattan Distance: Sum of absolute differences.
  - Cosine Similarity: Cosine of the angle between vectors.
- **Choosing the Right Metric:** Select based on data nature and clustering goals; experiment.

### Convergence of K-Means
- **Definition:** Occurs when cluster centroids stabilize between iterations.
- **Indicators of Convergence:**
  - Stable Centroids: No significant change in centroid positions (e.g., "stabilité 3 centres").
  - Assignment Stability: Points consistently belong to the same clusters.
  - Minimal Error Reduction: Small changes in the objective function.
- **Ensuring Convergence:** Use k-means++ for effective initial centroid selection; set a convergence threshold for centroid adjustments.

## 5. Practical Challenges and Solutions

### Common Issues with K-Means
- **Sensitivity to Initial Conditions:**
  - Clusters heavily depend on initial centroid selection, leading to variability.
  - Mitigation: Use k-means++ for better initial centroids.
- **Sensitivity to Outliers:**
  - Outliers can distort centroid calculation, resulting in biased clusters.
  - Mitigation: Use robust methods or consider algorithms like k-medoids.

### Improving K-Means Performance
- **Methods for Choosing Initial Centroids:**
  - k-means++: Optimizes initial centroid selection, leading to more consistent and efficient clustering, and reduces iterations.
- **Techniques for Outlier Handling:**
  - Data Cleaning: Remove noise and outliers before clustering.
  - Robust Scaling: Use scaling techniques less sensitive to outliers.

### Variants of K-Means
- **k-medoids:** Chooses actual data points (medoids) as centers, offering more robustness to outliers.
- **Fuzzy k-Means:** Each point belongs to all clusters with different degrees of membership, suitable for overlapping clusters.
- **k-means++:** (As mentioned above) Enhances the initialization phase of K-Means to improve cluster quality and convergence speed.
- **Bisecting k-Means:** A divisive clustering algorithm that iteratively splits clusters to achieve the desired number.

## 6. Advantages and Disadvantages of K-Means

### Advantages
- **Efficiency:** Operates in linear time, making it very fast.
- **Interpretability:** Clusters are easy to understand, centered around clear prototypes.
- **Simplicity:** Minimal parameters required (just the number of clusters and iterations).

### Disadvantages
- **Fixed k:** Number of clusters must be specified in advance.
- **Prototypical Constraints:** Limited to finding spherical-shaped clusters around centroids.
- **Initialization Sensitivity:** Outcome heavily depends on the initial position of centroids.