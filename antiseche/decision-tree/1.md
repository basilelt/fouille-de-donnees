# Notes sur les Arbres de Décision

Les arbres de décision sont une famille d'algorithmes de fouille de données très utilisés et interprétables, permettant la classification (prédiction de valeurs discrètes) et la régression (prédiction de valeurs continues). Leur nature visuelle et la facilité d'explication de leurs décisions en font un outil précieux, notamment pour la communication avec des experts non-informaticiens.

## 1. Qu'est-ce qu'un Arbre de Décision ?

- **Définition**: Une structure d'arbre où chaque composant a une fonction spécifique.
- **Structure**:
  - **Nœuds**: Représentent des tests sur les attributs du jeu de données.
  - **Branches**: Correspondent aux valeurs possibles que peut prendre un attribut.
  - **Feuilles**: Représentent une classe (pour la classification) ou une valeur continue (pour la régression).

## 2. Jeu de Données Exemple: Météo et Jeu

- **Contexte**: Prédiction de la participation à un jeu (ex: golf/tennis) en fonction des conditions météorologiques.
- **Attributs (exemples)**:
  - État du ciel (Sky)
  - Température (Temperature)
  - Humidité (Humidity)
  - Vent (Wind)
- **Classe à prédire**: Jouer (Yes/No).
- **Processus d'inférence**: Parcourir l'arbre en fonction des conditions météo pour arriver à une feuille (décision Yes/No).

## 3. Construction de l'Arbre de Décision: Algorithme ID3

- **Algorithme principal**: ID3 (Iterative Dichotomiser 3)
- **Proposé par**: Quinlan en 1986.
- **Approche**: top-down (descendante), itérative.
- **Concepts clés**:
  - **Entropie**: Mesure l'incertitude d'un ensemble de données.
    - Maximale quand il y a équiprobabilité (ex: 0.5 probabilité pour deux événements, entropie = 1).
    - Nulle quand il n'y a aucune incertitude (ex: 100% de "Yes").
  - **Gain d'information**: Mesure la réduction de l'incertitude sur la classification suite à la connaissance d'un attribut.
    - L'objectif est de trouver l'attribut qui offre le plus grand gain d'information à chaque étape.
- **Processus de construction (simplifié)**:
  1. Calculer l'entropie de la classe de la population totale (entropie initiale).
  2. Pour chaque attribut, calculer le gain d'information qu'il apporte.
  3. Choisir l'attribut avec le gain d'information le plus élevé comme nœud racine (ou nœud interne).
  4. Répéter le processus récursivement pour chaque branche, jusqu'à ce que les feuilles soient pures (toutes de la même classe) ou qu'un critère d'arrêt soit atteint.
- **Exemple de calcul initial** (9 Yes, 5 No): Entropie de la population ≈ 0.94.

## 4. Gestion des Valeurs Numériques

- **Problème**: Impossible de créer une branche pour chaque valeur continue possible (ex: température en degrés Fahrenheit, humidité en pourcentage).
- **Solution**: Discrétisation
  - Transformer une variable continue en variable discrète (ex: température < 70 = "froid", 70-75 = "doux", > 75 = "chaud").
  - Peut être effectuée en pré-traitement des données.
  - Certains algorithmes (ex: C4.5) intègrent la discrétisation et choisissent les seuils de coupe optimaux pendant la construction de l'arbre.

## 5. Surapprentissage (Overfitting)

- **Définition**: Le modèle adhère trop aux données d'entraînement, ce qui le rend incapable de bien généraliser sur de nouvelles données.
- **Causes spécifiques aux arbres de décision**:
  - Profondeur excessive de l'arbre.
  - Trop de petites coupes spécialisées sur les données d'entraînement.
  - Données d'entraînement insuffisantes.
  - Absence d'élagage (pruning).
- **Stratégies d'Élagage (Pruning) pour l'éviter**:
  - Supprimer les nœuds qui n'améliorent pas significativement les performances.
  - Ajouter des scores de pénalité pour les arbres trop complexes.
  - Utiliser la théorie MDL (Minimum Description Length) pour trouver la description minimale optimale.
  - Techniques d'élagage basées sur les erreurs.
  - Contrôler la profondeur maximale de l'arbre.
  - Pré-élagage (contraintes pendant la construction) vs. Post-élagage (modification de l'arbre après construction).

## 6. Forêts Aléatoires (Random Forests)

- **Concept**: Une collection d'arbres de décision (ensemble learning).
- **L'idée est de construire plusieurs arbres et de combiner leurs décisions/prédictions pour obtenir une prédiction finale.**
- **Avantages**:
  - Très performantes et largement utilisées.
  - Peuvent surpasser l'apprentissage profond sur certaines applications spécifiques.
- **Diversité des arbres**: Essentielle car la construction d'un arbre unique (ex: avec ID3) est déterministe.
- **Méthodes pour introduire la diversité**:
  - Utiliser différents algorithmes de construction.
  - Appliquer différentes méthodes d'élagage.
  - **Échantillonnage des données**:
    - Échantillonnage des instances (ne pas utiliser toutes les données pour chaque arbre).
    - Échantillonnage des attributs (ne pas utiliser tous les attributs pour chaque arbre).

## 7. Avantages et Inconvénients des Arbres de Décision

- **Avantages**:
  - Interprétables et explicables (très visuels).
  - Faible préparation des données (normalisation souvent intégrée).
  - Gestion des données numériques et catégorielles.
  - Capacité à représenter des frontières de classe non linéaires.
- **Désavantages**:
  - Surapprentissage fréquent (sans élagage).
  - Instabilité: Les petits changements dans les données peuvent entraîner des modifications significatives de l'arbre.
  - Complexité de mise à jour: Nécessite souvent une reconstruction complète de l'arbre lors de l'ajout de nouvelles données.
  - Biais: Peut survaloriser des attributs avec de nombreuses valeurs possibles.
  - Optimum local: L'algorithme glouton de construction ne garantit pas l'optimum global de l'arbre.

## 8. Conclusion

Les arbres de décision, bien que potentiellement instables et sujets au surapprentissage, restent un modèle très pertinent, surtout pour les petits volumes de données. Les Random Forests, leurs dérivés, sont des algorithmes extrêmement performants et très utilisés dans diverses applications.